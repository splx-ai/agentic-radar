{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from typing import Dict, Any, List, Union, Optional\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global dictionaries for everything:\n",
    "global_functions: Dict[str, Union[ast.FunctionDef, ast.AsyncFunctionDef]] = {}\n",
    "global_variables: Dict[str, ast.AST] = {}  # e.g., for top-level list/dict nodes\n",
    "\n",
    "def build_global_registry(root_dir: str, root_package: str) -> None:\n",
    "    \"\"\"\n",
    "    Recursively walk `root_dir` (containing your Python package),\n",
    "    parse each .py file to find top-level function defs and variable defs\n",
    "    (that are lists or dicts).\n",
    "\n",
    "    We'll store them in global_functions and global_variables with a fully\n",
    "    qualified name, e.g. \"some_package.module.foo_func\".\n",
    "    \"\"\"\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".py\"):\n",
    "                fullpath = os.path.join(dirpath, filename)\n",
    "                # Infer the module name from dirpath relative to root_dir, plus filename\n",
    "                module_name = derive_module_name(dirpath, filename, root_dir, root_package)\n",
    "                parse_for_top_level_defs(fullpath, module_name)\n",
    "\n",
    "def derive_module_name(dirpath: str, filename: str, root_dir: str, root_package: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a file path into a dotted module name. For example,\n",
    "    if root_dir = /path/to/my_project, root_package = \"my_project\",\n",
    "    and the file is /path/to/my_project/some_package/sub_module.py,\n",
    "    then we might return \"some_package.sub_module\".\n",
    "    \"\"\"\n",
    "    # This is somewhat custom depending on your directory structure.\n",
    "    # The general approach:\n",
    "    # 1. remove `root_dir` from the start of dirpath\n",
    "    # 2. split the remainder by os.sep\n",
    "    # 3. remove any empty strings or __pycache__\n",
    "    # 4. append filename without .py\n",
    "    # 5. join with '.'\n",
    "    rel_path = os.path.relpath(dirpath, root_dir)  # relative to the root\n",
    "    parts = []\n",
    "    if rel_path != \".\":\n",
    "        parts = rel_path.split(os.sep)\n",
    "\n",
    "    # remove .py extension from filename\n",
    "    base_name = filename[:-3]  # remove .py\n",
    "    parts.append(base_name)\n",
    "\n",
    "    # e.g. parts = [\"some_package\", \"sub_module\"]\n",
    "    # combine them with '.'\n",
    "    # If you want a leading \"my_project\" or \"root_package\", you can decide\n",
    "    # whether to keep that or not:\n",
    "    # For example, we might skip \"my_project\" if it's the top-level.\n",
    "    # Or we might do:\n",
    "    #    return \".\".join([root_package] + parts)\n",
    "    # depending on how you want the FQN to look.\n",
    "\n",
    "    module_name = \".\".join(parts)\n",
    "    return module_name\n",
    "\n",
    "\n",
    "def parse_for_top_level_defs(filepath: str, module_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Parse one file, collecting top-level function definitions and\n",
    "    top-level variable definitions (lists/dicts).\n",
    "    We store them in global_functions/global_variables with keys like:\n",
    "      \"some_package.sub_module.my_func\" -> ast.FunctionDef\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        source = f.read()\n",
    "    tree = ast.parse(source, filename=filepath)\n",
    "\n",
    "    # We'll define a small visitor:\n",
    "    class TopLevelCollector(ast.NodeVisitor):\n",
    "        def visit_FunctionDef(self, node: ast.FunctionDef):\n",
    "            # Build key\n",
    "            fq_key = f\"{module_name}.{node.name}\"\n",
    "            global_functions[fq_key] = node\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):\n",
    "            fq_key = f\"{module_name}.{node.name}\"\n",
    "            global_functions[fq_key] = node\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_Assign(self, node: ast.Assign):\n",
    "            # If it's a top-level assignment: e.g. my_var = [1,2,3]\n",
    "            # We'll store it if it's a list or dict\n",
    "            if isinstance(node.parent, ast.Module):\n",
    "                # We only want top-level assigns, not inside a class or function\n",
    "                if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):\n",
    "                    var_name = node.targets[0].id\n",
    "                    val = node.value\n",
    "                    if isinstance(val, (ast.List, ast.Dict)):\n",
    "                        fq_key = f\"{module_name}.{var_name}\"\n",
    "                        global_variables[fq_key] = val\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        # We also override generic_visit to set a parent pointer so we can detect top-level\n",
    "        def generic_visit(self, node):\n",
    "            for child in ast.iter_child_nodes(node):\n",
    "                child.parent = node  # store a pointer to parent\n",
    "                self.visit(child)\n",
    "\n",
    "    collector = TopLevelCollector()\n",
    "    collector.visit(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_global_registry(\"./Gladiator2\", \"./Gladiator2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "class ClassInstanceTracker(ast.NodeVisitor):\n",
    "    \"\"\"\n",
    "    A node visitor that:\n",
    "      1. Collects import information to map local names to fully-qualified paths.\n",
    "      2. Finds instantiations of a target class (by fully-qualified name).\n",
    "      3. Tracks method calls on those instances, capturing arguments.\n",
    "      4. Adds extra logic for a 'special method':\n",
    "         - If called with 2 positional args, the second is the name of a function -> gather return expressions\n",
    "         - If called with 3 positional args, the third is a literal list/dict or a variable referencing one -> gather values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph_class_fqcn: str, add_conditional_edges_method_name: str, add_node_method_name: str):\n",
    "        \"\"\"\n",
    "        :param graph_class_fqcn: The fully qualified class name of the Graph class to look for.\n",
    "        :param add_conditional_edges_method_name: The method name that, when called, triggers addition of conditional edges.\n",
    "        :param add_node_method_name: The method name that, when called, triggers addition of a node.\n",
    "        \"\"\"\n",
    "        self.graph_class_fqcn = graph_class_fqcn\n",
    "        self.add_conditional_edges_method_name = add_conditional_edges_method_name\n",
    "        self.add_node_method_name = add_node_method_name\n",
    "\n",
    "        # Maintain a mapping of local import aliases to fully qualified paths.\n",
    "        # E.g., \"import examples as ex\" => self.import_aliases[\"ex\"] = \"examples\"\n",
    "        self.import_aliases: Dict[str, str] = {}\n",
    "\n",
    "        # For \"from examples.graph import Graph\", store fully qualified references:\n",
    "        # e.g., self.import_aliases_fully[\"Graph\"] = \"examples.graph.Graph\"\n",
    "        self.import_aliases_fully: Dict[str, str] = {}\n",
    "\n",
    "        # We'll accumulate method calls in this structure:\n",
    "        # {\n",
    "        #     instance_name: {\n",
    "        #         method_name: [\n",
    "        #             {\n",
    "        #                 \"positional\": [...],\n",
    "        #                 \"keyword\": {...},\n",
    "        #                 # possibly \"function_returns\", \"list_values\", or \"dict_values\"\n",
    "        #             },\n",
    "        #             ...\n",
    "        #         ]\n",
    "        #     },\n",
    "        #     ...\n",
    "        # }\n",
    "        self.instance_methods: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}\n",
    "\n",
    "        # Track which variables are instances of our target class.\n",
    "        # E.g., if we see \"g = Graph()\", then self.variable_is_target_instance[\"g\"] = True\n",
    "        self.variable_is_target_instance: Dict[str, bool] = {}\n",
    "\n",
    "        # Keep track of all top-level functions (both async and regular):\n",
    "        # function name -> (ast.FunctionDef or ast.AsyncFunctionDef)\n",
    "        self.function_defs: Dict[str, Union[ast.FunctionDef, ast.AsyncFunctionDef]] = {}\n",
    "\n",
    "        # Track variables that hold a literal list or dict, so that if a method is called\n",
    "        # with that variable, we can retrieve its contents. E.g.:\n",
    "        # my_list = [1,2,3] => self.variable_values[\"my_list\"] = ast.List(...)\n",
    "        self.variable_values: Dict[str, Union[ast.List, ast.Dict]] = {}\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #                           AST Visitor Methods\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    def visit_Import(self, node: ast.Import) -> Any:\n",
    "        \"\"\"\n",
    "        Handle statements like:\n",
    "          import examples\n",
    "          import examples.graph as ex\n",
    "        We store the local alias => real module path in self.import_aliases.\n",
    "        \"\"\"\n",
    "        for alias in node.names:\n",
    "            local_name = alias.asname if alias.asname else alias.name\n",
    "            # e.g. \"examples.graph\" or \"examples\"\n",
    "            self.import_aliases[local_name] = alias.name\n",
    "\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ImportFrom(self, node: ast.ImportFrom) -> Any:\n",
    "        \"\"\"\n",
    "        Handle statements like:\n",
    "          from examples.graph import Graph\n",
    "          from examples import graph as gph\n",
    "        We store the local name => fully qualified path in self.import_aliases_fully.\n",
    "        \"\"\"\n",
    "        # node.module might be None for relative imports like \"from . import Something\"\n",
    "        if node.module is None:\n",
    "            # Skip or handle relative imports if needed\n",
    "            self.generic_visit(node)\n",
    "            return\n",
    "\n",
    "        base_module = node.module  # e.g. \"examples.graph\"\n",
    "        for alias in node.names:\n",
    "            local_name = alias.asname if alias.asname else alias.name\n",
    "            # e.g., local_name \"Graph\" -> \"examples.graph.Graph\"\n",
    "            self.import_aliases_fully[local_name] = f\"{base_module}.{alias.name}\"\n",
    "\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:\n",
    "        \"\"\"\n",
    "        Record the top-level function definitions for later retrieval.\n",
    "        \"\"\"\n",
    "        self.function_defs[node.name] = node\n",
    "        # Continue visiting its body, because it may have calls we care about\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:\n",
    "        \"\"\"\n",
    "        Same as visit_FunctionDef, but for async functions.\n",
    "        \"\"\"\n",
    "        self.function_defs[node.name] = node\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Assign(self, node: ast.Assign) -> Any:\n",
    "        \"\"\"\n",
    "        Handle assignments, e.g.:\n",
    "            g = Graph(...)\n",
    "            my_list = [1, 2, 3]\n",
    "            my_dict = {\"foo\": 42}\n",
    "        We identify if the assigned value is a call to our target class\n",
    "        or a literal list/dict for which we store references in variable_values.\n",
    "        \"\"\"\n",
    "        # If there's exactly one target, and it's a Name, e.g., \"x = ...\"\n",
    "        if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):\n",
    "            var_name = node.targets[0].id\n",
    "\n",
    "            # If the right side is a Call node, check if it instantiates our target class.\n",
    "            if isinstance(node.value, ast.Call):\n",
    "                if self._call_is_target_class(node.value):\n",
    "                    self.variable_is_target_instance[var_name] = True\n",
    "\n",
    "            # If the right side is a literal list or dict, store it for later.\n",
    "            if isinstance(node.value, (ast.List, ast.Dict)):\n",
    "                self.variable_values[var_name] = node.value\n",
    "            else:\n",
    "                # If we reassign the variable to something else, remove from variable_values\n",
    "                if var_name in self.variable_values:\n",
    "                    del self.variable_values[var_name]\n",
    "\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    # THIS CAN PROBABLY BE REMOVED\n",
    "    def visit_Expr(self, node: ast.Expr) -> Any:\n",
    "        \"\"\"\n",
    "        If there's a bare expression that directly calls something,\n",
    "        we check if it's instantiating our target class (rare, but possible).\n",
    "        \"\"\"\n",
    "        if isinstance(node.value, ast.Call):\n",
    "            self._call_is_target_class(node.value)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Call(self, node: ast.Call) -> Any:\n",
    "        \"\"\"\n",
    "        For method calls on an instance: e.g., g.add_node(...).\n",
    "        If 'g' is known to be an instance of our target class, we record the call.\n",
    "\n",
    "        Also, if the method name matches our 'special method', we run extra analysis:\n",
    "        - 2 args => second is a function name => gather return expressions\n",
    "        - 3 args => third is a literal list/dict or a variable referencing one => gather values\n",
    "        \"\"\"\n",
    "        # The 'func' of a Call might be an Attribute like g.add_node\n",
    "        if isinstance(node.func, ast.Attribute):\n",
    "            # node.func.value might be a Name, e.g., 'g'\n",
    "            if isinstance(node.func.value, ast.Name):\n",
    "                instance_name = node.func.value.id\n",
    "\n",
    "                # Check if that variable is an instance of our target class\n",
    "                if self.variable_is_target_instance.get(instance_name, False):\n",
    "                    # This is a method call on our target instance\n",
    "                    method_name = node.func.attr\n",
    "\n",
    "                    call_record = {\n",
    "                        \"positional\": [self._stringify_ast_node(a) for a in node.args],\n",
    "                        \"keyword\": {\n",
    "                            kw.arg: self._stringify_ast_node(kw.value)\n",
    "                            for kw in node.keywords\n",
    "                            if kw.arg is not None\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    # If it's the special method, do the extended analysis\n",
    "                    if method_name == self.add_conditional_edges_method_name:\n",
    "                        self._process_add_conditional_edges_method_call(node, call_record)\n",
    "\n",
    "                    if method_name == self.add_node_method_name:\n",
    "                        self._handle_add_node_argument(node, call_record)\n",
    "\n",
    "                    # Store the call record in instance_methods\n",
    "                    if instance_name not in self.instance_methods:\n",
    "                        self.instance_methods[instance_name] = {}\n",
    "                    if method_name not in self.instance_methods[instance_name]:\n",
    "                        self.instance_methods[instance_name][method_name] = []\n",
    "\n",
    "                    self.instance_methods[instance_name][method_name].append(call_record)\n",
    "\n",
    "        # Continue traversing the AST\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #                           Helper Methods\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    def _handle_add_node_argument(self, call_node: ast.Call, call_record: dict) -> None:\n",
    "        \"\"\"\n",
    "        If my_special_method has 1 positional arg => analyze that arg.\n",
    "        If my_special_method has 2 positional args => analyze the second arg.\n",
    "        We store the resulting info in call_record[\"node_definition_argument_info\"].\n",
    "        \"\"\"\n",
    "        num_pos = len(call_node.args)\n",
    "        if num_pos == 1:\n",
    "            # The single argument is call_node.args[0]\n",
    "            info = self._analyze_argument(call_node.args[0])\n",
    "            call_record[\"node_definition_argument_info\"] = info\n",
    "\n",
    "        elif num_pos == 2:\n",
    "            # The second argument is call_node.args[1]\n",
    "            info = self._analyze_argument(call_node.args[1])\n",
    "            call_record[\"node_definition_argument_info\"] = info\n",
    "\n",
    "\n",
    "    def _analyze_argument(self, node: ast.AST) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Return a dictionary describing the argument:\n",
    "        - \"original\": how it appears in the code\n",
    "        - \"fq_name\": best guess fully qualified name (if we can resolve)\n",
    "\n",
    "        The argument might be:\n",
    "        - A Name referencing a local or imported function/variable\n",
    "        - An Attribute referencing a local or imported function/variable\n",
    "        - A Call referencing a local or imported function/class\n",
    "        - (You could extend for other node types if needed.)\n",
    "        \"\"\"\n",
    "        result: Dict[str, Optional[str]] = {\n",
    "            \"original\": self._stringify_ast_node(node),\n",
    "            \"fq_name\": None\n",
    "        }\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1) If it's a bare name (like \"some_func\")\n",
    "        # -----------------------------------------\n",
    "        if isinstance(node, ast.Name):\n",
    "            fq_name = self._resolve_fq_name(node)\n",
    "            result[\"fq_name\"] = fq_name\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 2) If it's an attribute, e.g. \"some_import.func\"\n",
    "        # -------------------------------------------------\n",
    "        elif isinstance(node, ast.Attribute):\n",
    "            # e.g. node might represent \"some_import.function\"\n",
    "            fq_name = self._resolve_fq_name(node)\n",
    "            result[\"fq_name\"] = fq_name\n",
    "\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        # 3) If it's a call, e.g. \"some_import.function(...)\" or \"local_func(...)\" or \"Name()\"\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        elif isinstance(node, ast.Call):\n",
    "            # The function called might be a Name or an Attribute\n",
    "            func = node.func\n",
    "            if isinstance(func, (ast.Name, ast.Attribute)):\n",
    "                fq_name = self._resolve_fq_name(func)\n",
    "                result[\"fq_name\"] = fq_name\n",
    "\n",
    "            else:\n",
    "                # e.g. calling a lambda or subscript, etc.\n",
    "                result[\"fq_name\"] = result[\"original\"]\n",
    "\n",
    "        # If it's something else (List, Dict, Subscript, etc.), we skip. Extend as needed.\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def _record_function_info(self, arg_node: ast.AST, call_record: dict) -> None:\n",
    "        \"\"\"\n",
    "        Takes the node believed to represent a function reference, tries to figure out\n",
    "        if it's local or imported. We'll store info in call_record:\n",
    "            call_record[\"function_name\"]\n",
    "            call_record[\"function_source\"]\n",
    "            call_record[\"function_fq\"]  # fully qualified if we can determine\n",
    "        \"\"\"\n",
    "\n",
    "        # We'll do a small helper function to resolve the 'function reference' to an FQ name.\n",
    "        # Then see if it's local or imported or unknown.\n",
    "        function_fq_name = self._resolve_function_reference(arg_node)\n",
    "\n",
    "        if function_fq_name is None:\n",
    "            # We couldn't figure out a function name from this node\n",
    "            return\n",
    "\n",
    "        call_record[\"function_name\"] = function_fq_name  # or short name if you prefer\n",
    "\n",
    "        # Now decide if it's local or imported\n",
    "        # If the short name portion is in self.function_defs => local\n",
    "        # If it matches an import => imported\n",
    "        short_name = function_fq_name.split(\".\")[-1]\n",
    "\n",
    "        if short_name in self.function_defs:\n",
    "            call_record[\"function_source\"] = \"local\"\n",
    "        else:\n",
    "            call_record[\"function_source\"] = \"imported\"\n",
    "\n",
    "        call_record[\"function_fq\"] = function_fq_name\n",
    "\n",
    "    def _resolve_function_reference(self, node: ast.AST) -> Union[str, None]:\n",
    "        \"\"\"\n",
    "        Try to interpret 'node' as a reference to a function.\n",
    "        Possible cases:\n",
    "          - A bare Name, e.g. 'some_func'\n",
    "          - An attribute chain, e.g. 'utils.func'\n",
    "          - Something else (Call, Subscript, etc.)\n",
    "\n",
    "        We'll reuse our _resolve_fq_name logic if it's a Name or an Attribute.\n",
    "        If we can't handle it, return None.\n",
    "        \"\"\"\n",
    "        if isinstance(node, ast.Name):\n",
    "            # e.g. 'some_func'\n",
    "            # Check if it appears in import_aliases_fully or import_aliases\n",
    "            # or just return node.id as is\n",
    "            fq = self._resolve_fq_name(node)\n",
    "            return fq\n",
    "\n",
    "        elif isinstance(node, ast.Attribute):\n",
    "            # e.g. 'utils.func'\n",
    "            fq = self._resolve_fq_name(node)\n",
    "            return fq\n",
    "\n",
    "        # If it's something else (like a lambda, a call, a subscript),\n",
    "        # we won't treat it as a function reference for now\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "    def _process_add_conditional_edges_method_call(self, call_node: ast.Call, call_record: dict) -> None:\n",
    "        \"\"\"\n",
    "        If the method is our special method, do extra checks. Now we handle both\n",
    "        positional and keyword arguments in the source order to determine\n",
    "        the 'second' or 'third' argument.\n",
    "\n",
    "        1) If there are exactly 2 total arguments, treat the second as a function name => gather returns.\n",
    "        2) If there are exactly 3 total arguments, treat the third as a list/dict literal or variable => gather values.\n",
    "        \"\"\"\n",
    "        # Combine all arguments (positional + keyword) in source order.\n",
    "        # The 'value' of each keyword is the actual expression.\n",
    "        all_args_in_order = self._get_all_args_in_call(call_node)\n",
    "\n",
    "        if len(all_args_in_order) == 2:\n",
    "            # The second argument is presumably a function name\n",
    "            second_arg = all_args_in_order[1]\n",
    "            if isinstance(second_arg, ast.Name):\n",
    "                func_name = second_arg.id\n",
    "                # If we have that function in function_defs, gather return expressions\n",
    "                if func_name in self.function_defs:\n",
    "                    returns = self._get_return_expressions(self.function_defs[func_name])\n",
    "                    call_record[\"path\"] = {\n",
    "                        \"function_returns\": returns\n",
    "                    }\n",
    "                elif self._resolve_fq_name(second_arg) in global_functions:\n",
    "                    returns = self._get_return_expressions(global_functions[self._resolve_fq_name(second_arg)])\n",
    "                    call_record[\"path\"] = {\n",
    "                        \"function_returns\": returns\n",
    "                    }\n",
    "                else:\n",
    "                    # fq_name = self._resolve_fq_name(second_arg)\n",
    "                    # call_record[\"path\"] = {\n",
    "                    #     \"function_fq_name\": fq_name\n",
    "                    # }\n",
    "                    pass\n",
    "\n",
    "        elif len(all_args_in_order) == 3:\n",
    "            # The third argument might be a list/dict literal or a variable referencing one\n",
    "            third_arg = all_args_in_order[2]\n",
    "\n",
    "            # Directly a list literal\n",
    "            if isinstance(third_arg, ast.List):\n",
    "                list_vals = [self._stringify_ast_node(elt) for elt in third_arg.elts]\n",
    "                call_record[\"path_map\"] = {\n",
    "                    \"list_values\": list_vals\n",
    "                }\n",
    "\n",
    "            # Directly a dict literal\n",
    "            elif isinstance(third_arg, ast.Dict):\n",
    "                dict_vals = [self._stringify_ast_node(v) for v in third_arg.values]\n",
    "                call_record[\"path_map\"] = {\n",
    "                    \"dict_values\": dict_vals\n",
    "                }\n",
    "\n",
    "            # A variable referencing a list/dict literal\n",
    "            elif isinstance(third_arg, ast.Name):\n",
    "                var_name = third_arg.id\n",
    "                if var_name in self.variable_values:\n",
    "                    val_node = self.variable_values[var_name]\n",
    "                    if isinstance(val_node, ast.List):\n",
    "                        list_vals = [self._stringify_ast_node(elt) for elt in val_node.elts]\n",
    "                        call_record[\"path_map\"] = {\n",
    "                            \"list_values\": list_vals\n",
    "                        }\n",
    "                    elif isinstance(val_node, ast.Dict):\n",
    "                        dict_vals = [self._stringify_ast_node(v) for v in val_node.values]\n",
    "                        call_record[\"path_map\"] = {\n",
    "                            \"dict_values\": dict_vals\n",
    "                        }\n",
    "                elif self._resolve_fq_name(third_arg) in global_variables:\n",
    "                    val_node = global_variables[self._resolve_fq_name(third_arg)]\n",
    "                    if isinstance(val_node, ast.List):\n",
    "                        list_vals = [self._stringify_ast_node(elt) for elt in val_node.elts]\n",
    "                        call_record[\"path_map\"] = {\n",
    "                            \"list_values\": list_vals\n",
    "                        }\n",
    "                    elif isinstance(val_node, ast.Dict):\n",
    "                        dict_vals = [self._stringify_ast_node(v) for v in val_node.values]\n",
    "                        call_record[\"path_map\"] = {\n",
    "                            \"dict_values\": dict_vals\n",
    "                        }\n",
    "\n",
    "                else:\n",
    "                    # fq_name = self._resolve_fq_name(third_arg)\n",
    "                    # call_record[\"path_map\"] = {\n",
    "                    #     \"map_fq_name\": fq_name\n",
    "                    # }\n",
    "                    pass\n",
    "\n",
    "\n",
    "    def _call_is_target_class(self, call_node: ast.Call) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the call instantiates our target class, e.g. Graph(...) when we want\n",
    "        \"examples.graph.Graph\" or ex.graph.Graph(...) if ex => \"examples\".\n",
    "        We do this by resolving the fully qualified name of the callee and comparing\n",
    "        against self.graph_class_fqcn.\n",
    "        \"\"\"\n",
    "        fq_name = self._resolve_fq_name(call_node.func)\n",
    "        return (fq_name == self.graph_class_fqcn)\n",
    "\n",
    "    def _resolve_fq_name(self, node: Union[ast.Name, ast.Attribute]) -> str:\n",
    "        \"\"\"\n",
    "        Attempt to resolve a node (Name or Attribute chain) to a fully qualified name.\n",
    "        E.g., if node is \"ex.graph.Graph\" and ex => \"examples\", we get \"examples.graph.Graph\".\n",
    "        If it's just \"Graph\", and we have \"Graph\" => \"examples.graph.Graph\" in import_aliases_fully,\n",
    "        we return that. Otherwise, we return the best guess or the bare name.\n",
    "        \"\"\"\n",
    "        if isinstance(node, ast.Name):\n",
    "            # Possibly in self.import_aliases_fully or self.import_aliases\n",
    "            if node.id in self.import_aliases_fully:\n",
    "                return self.import_aliases_fully[node.id]\n",
    "            else:\n",
    "                # If not, check if there's a top-level import alias\n",
    "                return self.import_aliases.get(node.id, node.id)\n",
    "        elif isinstance(node, ast.Attribute):\n",
    "            # Build up from base, then dot + attr\n",
    "            base_fq = self._resolve_fq_name(node.value)\n",
    "            if base_fq:\n",
    "                return base_fq + \".\" + node.attr\n",
    "            else:\n",
    "                # Fallback: just the attribute name\n",
    "                return node.attr\n",
    "        return \"\"\n",
    "\n",
    "    def _stringify_ast_node(self, node: ast.AST) -> str:\n",
    "        \"\"\"\n",
    "        Convert an AST node into a string for storing in \"positional\"/\"keyword\" argument lists\n",
    "        or for analyzing returns. We do a few special cases for clarity:\n",
    "          - ast.Constant => repr(value)\n",
    "          - ast.List => \"ListLiteral([...])\"\n",
    "          - ast.Dict => \"DictLiteral({...})\"\n",
    "          - ast.Name => variable name\n",
    "          - ast.Call => \"Call(...ast dump...)\"\n",
    "          - Otherwise => ast.dump(node)\n",
    "        \"\"\"\n",
    "        if isinstance(node, ast.Constant):\n",
    "            if isinstance(node.value, str):\n",
    "                return node.value\n",
    "            return repr(node.value)\n",
    "        elif isinstance(node, ast.Name):\n",
    "            return node.id\n",
    "        elif isinstance(node, ast.Call):\n",
    "            return f\"Call({ast.dump(node)})\"\n",
    "        elif isinstance(node, ast.List):\n",
    "            # Convert elements recursively\n",
    "            return \"ListLiteral([\" + \", \".join(self._stringify_ast_node(elt) for elt in node.elts) + \"])\"\n",
    "        elif isinstance(node, ast.Dict):\n",
    "            # Convert key-value pairs recursively\n",
    "            keys = [self._stringify_ast_node(k) for k in node.keys]\n",
    "            vals = [self._stringify_ast_node(v) for v in node.values]\n",
    "            pairs_str = \", \".join(f\"{k}: {v}\" for k, v in zip(keys, vals))\n",
    "            return f\"DictLiteral({{{pairs_str}}})\"\n",
    "        else:\n",
    "            # Fallback: the raw AST dump\n",
    "            return ast.dump(node)\n",
    "        \n",
    "    def _get_all_args_in_call(self, call_node: ast.Call) -> List[ast.expr]:\n",
    "        \"\"\"\n",
    "        Returns a list of all arguments (positional first, then each\n",
    "        keyword argument's value) in the **exact order** they appear in the source.\n",
    "        \n",
    "        Example:\n",
    "          special_method(10, foo='bar', my_list=[1,2,3])\n",
    "          => [ast.Constant(value=10), ast.Constant(value='bar'), ast.List(elts=[...])]\n",
    "        \n",
    "        This helps unify the logic for \"the second argument,\" \"the third argument,\" etc.\n",
    "        \"\"\"\n",
    "        combined = []\n",
    "        combined.extend(call_node.args)\n",
    "\n",
    "        for kw in call_node.keywords:\n",
    "            combined.append(kw.value)\n",
    "        return combined\n",
    "\n",
    "    def _get_return_expressions(\n",
    "        self,\n",
    "        func_def: Union[ast.FunctionDef, ast.AsyncFunctionDef]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Traverse the body of a function definition and collect stringified return expressions.\n",
    "        For every 'return X', we gather the stringified form of X.\n",
    "        \"\"\"\n",
    "\n",
    "        returns = []\n",
    "\n",
    "        class ReturnCollector(ast.NodeVisitor):\n",
    "            \"\"\"\n",
    "            Inner visitor class that visits Return nodes in the function body.\n",
    "            We store them in the outer 'returns' list.\n",
    "            \"\"\"\n",
    "\n",
    "            def __init__(inner_self, outer_self):\n",
    "                super().__init__()\n",
    "                inner_self.outer = outer_self\n",
    "                inner_self.variables: Dict[str, str] = {}\n",
    "\n",
    "            def visit_Return(inner_self, return_node: ast.Return):\n",
    "                if return_node.value is not None and isinstance(return_node.value, ast.Name):\n",
    "                    if return_node.value.id in inner_self.variables:\n",
    "                        returns.extend(inner_self.variables[return_node.value.id])\n",
    "                    else:\n",
    "                        returns.append(inner_self.outer._stringify_ast_node(return_node.value))\n",
    "                if return_node.value is not None and isinstance(return_node.value, ast.Constant):\n",
    "                    # Call the outer class's _stringify_ast_node to convert the expression\n",
    "                    returns.append(return_node.value.value)\n",
    "\n",
    "            def visit_Assign(inner_self, node: ast.Assign) -> Any:\n",
    "                \"\"\"\n",
    "                Handle assignments in case code is like:\n",
    "                if ...:\n",
    "                    next_node = \"node1\"\n",
    "                else:\n",
    "                    next_node = \"node2\"\n",
    "\n",
    "                return next_node\n",
    "                \"\"\"\n",
    "                # If there's exactly one target, and it's a Name, e.g., \"x = ...\"\n",
    "                if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):\n",
    "                    var_name = node.targets[0].id\n",
    "\n",
    "                    # If the right side is a literal list or dict, store it for later.\n",
    "                    if isinstance(node.value, ast.Constant):\n",
    "                        # and node.value accounts for None\n",
    "                        if var_name not in inner_self.variables and node.value.value is not None:\n",
    "                            inner_self.variables[var_name] = [node.value.value]\n",
    "                        elif node.value.value:\n",
    "                            inner_self.variables[var_name].append(node.value.value)\n",
    "                    elif isinstance(node.value, ast.Name):\n",
    "                        if var_name not in inner_self.variables and node.value.value is not None:\n",
    "                            inner_self.variables[var_name] = [inner_self.outer._stringify_ast_node(node.value)]\n",
    "                        elif node.value:\n",
    "                            inner_self.variables[var_name].append(inner_self.outer._stringify_ast_node(node.value))\n",
    "\n",
    "\n",
    "        # Instantiate the ReturnCollector, giving it a reference to our outer class (self)\n",
    "        ReturnCollector(self).visit(func_def)\n",
    "        return returns\n",
    "\n",
    "\n",
    "def parse_python_file(\n",
    "    filepath: str,\n",
    "    target_class: str,\n",
    "    add_conditional_edges_method_name: str,\n",
    "    add_node_method_name: str\n",
    ") -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Parse a single Python file for:\n",
    "      - Instantiations of 'target_class'\n",
    "      - Method calls on those instances\n",
    "      - Special logic for the specified 'add_conditional_edges_method_name'\n",
    "\n",
    "    :param filepath: path to a .py file\n",
    "    :param target_class: fully-qualified class name we want to detect\n",
    "    :param add_conditional_edges_method_name: name of the method that triggers extra analysis\n",
    "    :return: structure summarizing the method calls on each instance\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        source = f.read()\n",
    "\n",
    "    # Parse the entire file into an AST\n",
    "    tree = ast.parse(source, filename=filepath)\n",
    "\n",
    "    # Create our tracking visitor\n",
    "    tracker = ClassInstanceTracker(target_class, add_conditional_edges_method_name, add_node_method_name)\n",
    "\n",
    "    # Visit the AST\n",
    "    tracker.visit(tree)\n",
    "\n",
    "    # Return the final dictionary of instance methods\n",
    "    return tracker.instance_methods\n",
    "\n",
    "\n",
    "def walk_directory_and_parse(\n",
    "    root_dir: str,\n",
    "    target_class: str,\n",
    "    add_conditional_edges_method_name: str,\n",
    "    add_node_method_name: str\n",
    ") -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Recursively walk a directory (root_dir), parse each .py file,\n",
    "    and merge results for the given target class & special method logic.\n",
    "\n",
    "    :param root_dir: The directory path to recursively walk\n",
    "    :param target_class: Fully qualified class name, e.g. \"examples.graph.Graph\"\n",
    "    :param add_conditional_edges_method_name: The method name that triggers extra analysis\n",
    "    :param add_node_method_name: The method name that triggers extra analysis\n",
    "    :return: Combined dictionary for all .py files under root_dir\n",
    "    \"\"\"\n",
    "    combined_results: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".py\"):\n",
    "                fullpath = os.path.join(dirpath, filename)\n",
    "                file_results = parse_python_file(fullpath, target_class, add_conditional_edges_method_name, add_node_method_name)\n",
    "\n",
    "                # Merge this file's results into the overall dictionary\n",
    "                for instance_name, methods_dict in file_results.items():\n",
    "                    if instance_name not in combined_results:\n",
    "                        combined_results[instance_name] = {}\n",
    "                    for method_name, calls in methods_dict.items():\n",
    "                        if method_name not in combined_results[instance_name]:\n",
    "                            combined_results[instance_name][method_name] = []\n",
    "                        combined_results[instance_name][method_name].extend(calls)\n",
    "                        combined_results[instance_name][\"filepath\"] = fullpath\n",
    "\n",
    "    return combined_results\n",
    "\n",
    "# root_directory = \"./customer_service\"\n",
    "# graph_class_fqcn = \"langgraph.graph.StateGraph\"\n",
    "# add_conditional_edges_method_name = \"add_conditional_edges\"\n",
    "# add_node_method_name = \"add_node\"\n",
    "\n",
    "root_directory = \"./Gladiator2\"\n",
    "graph_class_fqcn = \"langgraph.graph.StateGraph\"\n",
    "add_conditional_edges_method_name = \"add_conditional_edges\"\n",
    "add_node_method_name = \"add_node\"\n",
    "\n",
    "results = walk_directory_and_parse(root_directory, graph_class_fqcn, add_conditional_edges_method_name, add_node_method_name)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_custom_tools_with_ast(file_content, file_path):\n",
    "    custom_tools = []\n",
    "    tree = ast.parse(file_content)\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            decorator_names = []\n",
    "            for decorator in node.decorator_list:\n",
    "                if isinstance(decorator, ast.Name):\n",
    "                    decorator_names.append(decorator.id)\n",
    "                elif isinstance(decorator, ast.Call):\n",
    "                    if isinstance(decorator.func, ast.Name):\n",
    "                        decorator_names.append(decorator.func.id)\n",
    "                    elif isinstance(decorator.func, ast.Attribute):\n",
    "                        decorator_names.append(decorator.func.attr)\n",
    "            \n",
    "            if \"tool\" in decorator_names:\n",
    "                custom_tools.append({\n",
    "                    \"name\": node.name,\n",
    "                    \"filepath\": file_path\n",
    "                })\n",
    "    \n",
    "    return custom_tools\n",
    "    \n",
    "\n",
    "\n",
    "results = {}\n",
    "def parse_all_custom_tools_from_directory(directory_path):\n",
    "    all_custom_tools = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    file_content = f.read()\n",
    "                    custom_tools = extract_custom_tools_with_ast(file_content, file_path)\n",
    "                    for single_custom_tool in custom_tools:\n",
    "                        all_custom_tools.append(single_custom_tool)\n",
    "    \n",
    "    return all_custom_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imports_with_ast(file_content):\n",
    "    imports = []\n",
    "    tree = ast.parse(file_content)\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for alias in node.names:\n",
    "                full_import = alias.name\n",
    "                imports.append(full_import)\n",
    "        \n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            module = node.module\n",
    "            for alias in node.names:\n",
    "                full_import = f\"{module}.{alias.name}\"\n",
    "                imports.append(full_import)\n",
    "    \n",
    "    return imports\n",
    "\n",
    "results = {}\n",
    "def parse_all_imports_from_directory(directory_path):\n",
    "    all_imports = set()\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    file_content = f.read()\n",
    "                    imports = extract_imports_with_ast(file_content)\n",
    "                    for single_import in imports:\n",
    "                        all_imports.add(single_import)\n",
    "    \n",
    "    return all_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predefined_tools_dict = {'AINetwork Toolkit': {'imports': 'from langchain_community.agent_toolkits.ainetwork.toolkit import AINetworkToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.ainetwork.toolkit.AINetworkToolkit']}, 'Alpha Vantage': {'imports': 'from langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.alpha_vantage.AlphaVantageAPIWrapper']}, 'Amadeus Toolkit': {'imports': 'from langchain_community.agent_toolkits.amadeus.toolkit import AmadeusToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.amadeus.toolkit.AmadeusToolkit']}, 'ArXiv': {'imports': 'from langchain_community.utilities import ArxivAPIWrapper\\nor\\nfrom langchain.agents import load_tools\\ntools = load_tools(\\n    [\"arxiv\"],\\n)', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.ArxivAPIWrapper', 'langchain.agents.load_tools']}, 'AskNews': {'imports': 'from langchain_community.retrievers import AskNewsRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.AskNewsRetriever']}, 'AWS Lambda': {'imports': 'from langchain.agents import load_tools\\ntools = load_tools(\\n    [\"awslambda\"],\\n)', 'category': 'GENERAL', 'import_list': ['langchain.agents.load_tools']}, 'Azure AI Services Toolkit': {'imports': 'from langchain_community.agent_toolkits import AzureAiServicesToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.AzureAiServicesToolkit']}, 'Azure Cognitive Services Toolkit': {'imports': 'from langchain_community.agent_toolkits import AzureCognitiveServicesToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.AzureCognitiveServicesToolkit']}, 'Azure Container Apps dynamic sessions': {'imports': 'from langchain_azure_dynamic_sessions import SessionsPythonREPLTool', 'category': 'GENERAL', 'import_list': ['langchain_azure_dynamic_sessions.SessionsPythonREPLTool']}, 'Shell (bash)': {'imports': 'from langchain_community.tools import ShellTool', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.ShellTool']}, 'Bearly Code Interpreter': {'imports': 'from langchain_community.tools import BearlyInterpreterTool', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.BearlyInterpreterTool']}, 'Bing Search': {'imports': 'from langchain_community.utilities import BingSearchAPIWrapper\\nor\\nfrom langchain_community.tools.bing_search import BingSearchResults\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.BingSearchAPIWrapper', 'langchain_community.tools.bing_search.BingSearchResults']}, 'Brave Search': {'imports': 'from langchain_community.document_loaders import BraveSearchLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.BraveSearchLoader']}, 'Cassandra Database Toolkit': {'imports': 'from langchain_community.agent_toolkits.cassandra_database.toolkit import CassandraDatabaseToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.cassandra_database.toolkit.CassandraDatabaseToolkit']}, 'CDP': {'imports': 'from cdp_langchain.agent_toolkits import CdpToolkit\\nor\\nfrom cdp_langchain.utils import CdpAgentkitWrapper\\n', 'category': 'GENERAL', 'import_list': ['cdp_langchain.agent_toolkits.CdpToolkit', 'cdp_langchain.utils.CdpAgentkitWrapper']}, 'ClickUp Toolkit': {'imports': 'from langchain_community.agent_toolkits.clickup.toolkit import ClickupToolkit\\nor\\nfrom langchain_community.utilities.clickup import ClickupAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.clickup.toolkit.ClickupToolkit', 'langchain_community.utilities.clickup.ClickupAPIWrapper']}, 'Cogniswitch Toolkit': {'imports': 'from langchain_community.agent_toolkits import CogniswitchToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.CogniswitchToolkit']}, 'Connery Toolkit and Tools': {'imports': 'from langchain_community.agent_toolkits.connery import ConneryToolkit\\nor\\nfrom langchain_community.tools.connery import ConneryService', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.connery.ConneryToolkit', 'langchain_community.tools.connery.ConneryService']}, 'Dall-E Image Generator': {'imports': 'from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nor\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"dalle-image-generator\"])\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.dalle_image_generator.DallEAPIWrapper', 'langchain.agents.load_tools']}, 'Dappier': {'imports': 'from langchain_dappier import DappierRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_dappier.DappierRetriever']}, 'Databricks Unity Catalog (UC)': {'imports': 'from langchain_community.tools.databricks import UCFunctionToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.databricks.UCFunctionToolkit']}, 'DataForSEO': {'imports': 'from langchain_community.utilities.dataforseo_api_search import DataForSeoAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.dataforseo_api_search.DataForSeoAPIWrapper']}, 'Dataherald': {'imports': 'from langchain_community.utilities.dataherald import DataheraldAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.dataherald.DataheraldAPIWrapper']}, 'DuckDuckGo Search': {'imports': 'from langchain_community.tools import DuckDuckGoSearchRun\\nor\\nfrom langchain_community.tools import DuckDuckGoSearchResults', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.DuckDuckGoSearchRun', 'langchain_community.tools.DuckDuckGoSearchResults']}, 'E2B Data Analysis': {'imports': 'from langchain_community.tools import E2BDataAnalysisTool', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.E2BDataAnalysisTool']}, 'Eden AI': {'imports': 'from langchain_community.chat_models.edenai import ChatEdenAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.edenai.ChatEdenAI']}, 'Eleven Labs Text2Speech': {'imports': 'from langchain_community.tools import ElevenLabsText2SpeechTool\\nor\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"eleven_labs_text2speech\"])', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.ElevenLabsText2SpeechTool', 'langchain.agents.load_tools']}, 'Exa Search': {'imports': 'from exa_py import Exa', 'category': 'GENERAL', 'import_list': ['exa_py.Exa']}, 'File System': {'imports': 'from langchain_community.agent_toolkits import FileManagementToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.FileManagementToolkit']}, 'FinancialDatasets Toolkit': {'imports': 'from langchain_community.agent_toolkits.financial_datasets.toolkit import FinancialDatasetsToolkit\\nor\\nfrom langchain_community.utilities.financial_datasets import FinancialDatasetsAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.financial_datasets.toolkit.FinancialDatasetsToolkit', 'langchain_community.utilities.financial_datasets.FinancialDatasetsAPIWrapper']}, 'FMP Data': {'imports': 'from langchain_fmp_data import FMPDataTool\\nor\\nfrom langchain_fmp_data import FMPDataToolkit', 'category': 'GENERAL', 'import_list': ['langchain_fmp_data.FMPDataTool', 'langchain_fmp_data.FMPDataToolkit']}, 'Github Toolkit': {'imports': 'from langchain_community.agent_toolkits.github.toolkit import GitHubToolkit\\nor\\nfrom langchain_community.utilities.github import GitHubAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.github.toolkit.GitHubToolkit', 'langchain_community.utilities.github.GitHubAPIWrapper']}, 'Gitlab Toolkit': {'imports': 'from langchain_community.agent_toolkits.gitlab.toolkit import GitLabToolkit\\nor\\nfrom langchain_community.utilities.gitlab import GitLabAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.gitlab.toolkit.GitLabToolkit', 'langchain_community.utilities.gitlab.GitLabAPIWrapper']}, 'Gmail Toolkit': {'imports': 'from langchain_google_community import GmailToolkit', 'category': 'GENERAL', 'import_list': ['langchain_google_community.GmailToolkit']}, 'Golden Query': {'imports': 'from langchain_community.utilities.golden_query import GoldenQueryAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.golden_query.GoldenQueryAPIWrapper']}, 'Google Books': {'imports': 'from langchain_community.tools.google_books import GoogleBooksQueryRun\\nor\\nfrom langchain_community.utilities.google_books import GoogleBooksAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.google_books.GoogleBooksQueryRun', 'langchain_community.utilities.google_books.GoogleBooksAPIWrapper']}, 'Google Cloud Text-to-Speech': {'imports': 'from langchain_google_community import TextToSpeechTool', 'category': 'GENERAL', 'import_list': ['langchain_google_community.TextToSpeechTool']}, 'Google Drive': {'imports': 'from langchain_google_community import GoogleDriveLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_community.GoogleDriveLoader']}, 'Google Finance': {'imports': 'from langchain_community.tools.google_finance import GoogleFinanceQueryRun\\nor\\nfrom langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\\nor\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"google-finance\"])', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.google_finance.GoogleFinanceQueryRun', 'langchain_community.utilities.google_finance.GoogleFinanceAPIWrapper', 'langchain.agents.load_tools']}, 'Google Imagen': {'imports': 'from langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat\\nor\\nfrom langchain_google_vertexai.vision_models import VertexAIImageEditorChat\\nor\\nfrom langchain_google_vertexai import VertexAIImageCaptioning\\nor\\nfrom langchain_google_vertexai import VertexAIVisualQnAChat', 'category': 'GENERAL', 'import_list': ['langchain_google_vertexai.vision_models.VertexAIImageGeneratorChat', 'langchain_google_vertexai.vision_models.VertexAIImageEditorChat', 'langchain_google_vertexai.VertexAIImageCaptioning', 'langchain_google_vertexai.VertexAIVisualQnAChat']}, 'Google Jobs': {'imports': 'from langchain_community.tools.google_jobs import GoogleJobsQueryRun\\nor\\nfrom langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper\\nor\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"google-jobs\"], llm=llm)', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.google_jobs.GoogleJobsQueryRun', 'langchain_community.utilities.google_jobs.GoogleJobsAPIWrapper', 'langchain.agents.load_tools']}, 'Google Lens': {'imports': 'from langchain_community.tools.google_lens import GoogleLensQueryRun\\nor\\nfrom langchain_community.utilities.google_lens import GoogleLensAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.google_lens.GoogleLensQueryRun', 'langchain_community.utilities.google_lens.GoogleLensAPIWrapper']}, 'Google Places': {'imports': 'from langchain_community.tools import GooglePlacesTool', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.GooglePlacesTool']}, 'Google Scholar': {'imports': 'from langchain_community.tools.google_scholar import GoogleScholarQueryRun\\nor\\nfrom langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.google_scholar.GoogleScholarQueryRun', 'langchain_community.utilities.google_scholar.GoogleScholarAPIWrapper']}, 'Google Search': {'imports': 'from langchain_google_community import GoogleSearchAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_google_community.GoogleSearchAPIWrapper']}, 'Google Serper': {'imports': 'from langchain_community.utilities import GoogleSerperAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.GoogleSerperAPIWrapper']}, 'Google Trends': {'imports': 'from langchain_community.tools.google_trends import GoogleTrendsQueryRun\\nor\\nfrom langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.google_trends.GoogleTrendsQueryRun', 'langchain_community.utilities.google_trends.GoogleTrendsAPIWrapper']}, 'Gradio': {'imports': 'from gradio_tools.tools import ImageCaptioningTool\\nor\\nfrom gradio_tools.tools import StableDiffusionPromptGeneratorTool\\nor\\nfrom gradio_tools.tools import StableDiffusionTool\\nor\\nfrom gradio_tools.tools import TextToVideoTool', 'category': 'GENERAL', 'import_list': ['gradio_tools.tools.ImageCaptioningTool', 'gradio_tools.tools.StableDiffusionPromptGeneratorTool', 'gradio_tools.tools.StableDiffusionTool', 'gradio_tools.tools.TextToVideoTool']}, 'GraphQL': {'imports': 'from langchain.agents import load_tools\\ntools = load_tools(\\n    [\"graphql\"],\\n)', 'category': 'GENERAL', 'import_list': ['langchain.agents.load_tools']}, 'HuggingFace Hub Tools': {'imports': 'from langchain_community.agent_toolkits.load_tools import load_huggingface_tool', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.load_tools.load_huggingface_tool']}, 'Human as a tool': {'imports': 'from langchain.agents import load_tools\\ntools = load_tools(\\n    [\"human\"],\\n)', 'category': 'GENERAL', 'import_list': ['langchain.agents.load_tools']}, 'IFTTT WebHooks': {'imports': 'from langchain_community.tools.ifttt import IFTTTWebhook', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.ifttt.IFTTTWebhook']}, 'Infobip': {'imports': 'from langchain_community.utilities.infobip import InfobipAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.infobip.InfobipAPIWrapper']}, 'Ionic Shopping Tool': {'imports': 'from ionic_langchain.tool import IonicTool', 'category': 'GENERAL', 'import_list': ['ionic_langchain.tool.IonicTool']}, 'Jina Search': {'imports': 'from langchain_community.tools import JinaSearch', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.JinaSearch']}, 'Jira Toolkit': {'imports': 'from langchain_community.agent_toolkits.jira.toolkit import JiraToolkit\\nor\\nfrom langchain_community.utilities.jira import JiraAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.jira.toolkit.JiraToolkit', 'langchain_community.utilities.jira.JiraAPIWrapper']}, 'JSON Toolkit': {'imports': 'from langchain_community.agent_toolkits import JsonToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.JsonToolkit']}, 'Lemon Agent': {'imports': 'from lemonai import execute_workflow', 'category': 'GENERAL', 'import_list': ['lemonai.execute_workflow']}, 'LinkupSearchTool': {'imports': 'from langchain_linkup import LinkupSearchTool', 'category': 'GENERAL', 'import_list': ['langchain_linkup.LinkupSearchTool']}, 'Memorize': {'imports': 'from langchain.agents import load_tools\\ntools = load_tools([\"memorize\"], llm=llm)', 'category': 'GENERAL', 'import_list': ['langchain.agents.load_tools']}, 'Mojeek Search': {'imports': 'from langchain_community.tools import MojeekSearch', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.MojeekSearch']}, 'MultiOn Toolkit': {'imports': 'from langchain_community.agent_toolkits import MultionToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.MultionToolkit']}, 'NASA Toolkit': {'imports': 'from langchain_community.agent_toolkits.nasa.toolkit import NasaToolkit\\nor\\nfrom langchain_community.utilities.nasa import NasaAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.nasa.toolkit.NasaToolkit', 'langchain_community.utilities.nasa.NasaAPIWrapper']}, 'Nuclia': {'imports': 'from langchain_community.document_loaders.nuclia import NucliaLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.nuclia.NucliaLoader']}, 'NVIDIA RIVA': {'imports': 'from langchain_community.utilities.nvidia_riva import RivaASR\\nor\\nfrom langchain_community.utilities.nvidia_riva import RivaTTS', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.nvidia_riva.RivaASR', 'langchain_community.utilities.nvidia_riva.RivaTTS']}, 'Office365 Toolkit': {'imports': 'from langchain_community.agent_toolkits import O365Toolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.O365Toolkit']}, 'OpenAPI Toolkit': {'imports': 'from langchain_community.agent_toolkits import OpenAPIToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.OpenAPIToolkit']}, 'Natural Language API Toolkit': {'imports': 'from langchain_community.agent_toolkits import NLAToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.NLAToolkit']}, 'OpenWeatherMap': {'imports': 'from langchain_community.utilities import OpenWeatherMapAPIWrapper\\nor\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"openweathermap-api\"])', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.OpenWeatherMapAPIWrapper', 'langchain.agents.load_tools']}, 'Oracle AI Vector Search Generate Summary': {'imports': 'from langchain_community.utilities.oracleai import OracleSummary', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.oracleai.OracleSummary']}, 'Pandas Dataframe': {'imports': 'from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent', 'category': 'GENERAL', 'import_list': ['langchain_experimental.agents.agent_toolkits.create_pandas_dataframe_agent']}, 'Passio NutritionAI': {'imports': 'from langchain_community.tools.passio_nutrition_ai import NutritionAI\\nor\\nfrom langchain_community.utilities.passio_nutrition_ai import NutritionAIAPI\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.passio_nutrition_ai.NutritionAI', 'langchain_community.utilities.passio_nutrition_ai.NutritionAIAPI']}, 'PaymanAI': {'imports': 'from langchain_community.tools.langchain_payman_tool.tool import PaymanAI', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.langchain_payman_tool.tool.PaymanAI']}, 'PlayWright Browser Toolkit': {'imports': 'from langchain_community.agent_toolkits import PlayWrightBrowserToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.PlayWrightBrowserToolkit']}, 'Polygon IO Toolkit': {'imports': 'from langchain_community.agent_toolkits.polygon.toolkit import PolygonToolkit\\nor\\nfrom langchain_community.utilities.polygon import PolygonAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.polygon.toolkit.PolygonToolkit', 'langchain_community.utilities.polygon.PolygonAPIWrapper']}, 'PowerBI Toolkit': {'imports': 'from langchain_community.agent_toolkits import PowerBIToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.PowerBIToolkit']}, 'Polygon Stock Market API': {'imports': 'from langchain.tools.polygon import PolygonStockMarket', 'category': 'GENERAL', 'import_list': ['langchain.tools.polygon.PolygonStockMarket']}, 'PubMed': {'imports': 'from langchain_community.document_loaders import PubMedLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PubMedLoader']}, 'Python REPL': {'imports': 'from langchain_experimental.utilities import PythonREPL', 'category': 'GENERAL', 'import_list': ['langchain_experimental.utilities.PythonREPL']}, 'Reddit Search': {'imports': 'from langchain_community.tools.reddit_search.tool import RedditSearchRun\\nor\\nfrom langchain_community.utilities.reddit_search import RedditSearchAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.reddit_search.tool.RedditSearchRun', 'langchain_community.utilities.reddit_search.RedditSearchAPIWrapper']}, 'Requests': {'imports': 'from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.openapi.toolkit.RequestsToolkit']}, 'Riza Code Interpreter': {'imports': 'from langchain_community.tools.riza.command import ExecPython', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.riza.command.ExecPython']}, 'Robocorp Toolkit': {'imports': 'from langchain_robocorp import ActionServerToolkit', 'category': 'GENERAL', 'import_list': ['langchain_robocorp.ActionServerToolkit']}, 'SceneXplain': {'imports': 'from langchain_community.tools import SceneXplainTool\\nor\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"sceneXplain\"])', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.SceneXplainTool', 'langchain.agents.load_tools']}, 'ScrapeGraph': {'imports': 'from langchain_scrapegraph.tools import GetCreditsTool\\nor\\nfrom langchain_scrapegraph.tools import LocalScraperTool\\nor\\nfrom langchain_scrapegraph.tools import MarkdownifyTool\\nor\\nfrom langchain_scrapegraph.tools import SmartScraperTool', 'category': 'GENERAL', 'import_list': ['langchain_scrapegraph.tools.GetCreditsTool', 'langchain_scrapegraph.tools.LocalScraperTool', 'langchain_scrapegraph.tools.MarkdownifyTool', 'langchain_scrapegraph.tools.SmartScraperTool']}, 'SearchApi': {'imports': 'from langchain_community.utilities import SearchApiAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.SearchApiAPIWrapper']}, 'SearxNG Search': {'imports': 'from langchain_community.utilities import SearxSearchWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.SearxSearchWrapper']}, 'Semantic Scholar API': {'imports': 'from langchain_community.tools.semanticscholar.tool import SemanticScholarQueryRun', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.semanticscholar.tool.SemanticScholarQueryRun']}, 'SerpAPI': {'imports': 'from langchain_community.utilities import SerpAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.SerpAPIWrapper']}, 'Slack Toolkit': {'imports': 'from langchain_community.agent_toolkits import SlackToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.SlackToolkit']}, 'Spark SQL Toolkit': {'imports': 'from langchain_community.agent_toolkits import SparkSQLToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.SparkSQLToolkit']}, 'SQLDatabase Toolkit': {'imports': 'from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.sql.toolkit.SQLDatabaseToolkit']}, 'StackExchange': {'imports': 'from langchain_community.utilities import StackExchangeAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.StackExchangeAPIWrapper']}, 'Steam Toolkit': {'imports': 'from langchain_community.agent_toolkits.steam.toolkit import SteamToolkit\\nor\\nfrom langchain_community.utilities.steam import SteamWebAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.steam.toolkit.SteamToolkit', 'langchain_community.utilities.steam.SteamWebAPIWrapper']}, 'Stripe': {'imports': 'from langchain_community.document_loaders import StripeLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.StripeLoader']}, 'Tavily Search': {'imports': 'from langchain_community.tools import TavilySearchResults', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.TavilySearchResults']}, 'Tilores': {'imports': 'from tilores import TiloresAPI\\nor\\nfrom tilores_langchain import TiloresTools\\n', 'category': 'GENERAL', 'import_list': ['tilores.TiloresAPI', 'tilores_langchain.TiloresTools']}, 'Twilio': {'imports': 'from langchain_community.utilities.twilio import TwilioAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.twilio.TwilioAPIWrapper']}, 'Upstage Groundedness Check': {'imports': 'from langchain_upstage import UpstageGroundednessCheck', 'category': 'GENERAL', 'import_list': ['langchain_upstage.UpstageGroundednessCheck']}, 'Wikidata': {'imports': 'from langchain_community.tools.wikidata.tool import WikidataAPIWrapper\\nor\\nfrom langchain_community.tools.wikidata.tool import WikidataQueryRun\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.wikidata.tool.WikidataAPIWrapper', 'langchain_community.tools.wikidata.tool.WikidataQueryRun']}, 'Wikipedia': {'imports': 'from langchain_community.document_loaders import WikipediaLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.WikipediaLoader']}, 'Wolfram Alpha': {'imports': 'from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper', 'category': 'GENERAL', 'import_list': ['langchain_community.utilities.wolfram_alpha.WolframAlphaAPIWrapper']}, 'Yahoo Finance News': {'imports': 'from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.yahoo_finance_news.YahooFinanceNewsTool']}, 'You.com Search': {'imports': 'from langchain_community.tools.you import YouSearchTool\\nor\\nfrom langchain_community.utilities.you import YouSearchAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.you.YouSearchTool', 'langchain_community.utilities.you.YouSearchAPIWrapper']}, 'YouTube': {'imports': 'from langchain_community.tools import YouTubeSearchTool', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.YouTubeSearchTool']}, 'Zapier Natural Language Actions': {'imports': 'from langchain_community.agent_toolkits import ZapierToolkit\\nor\\nfrom langchain_community.utilities.zapier import ZapierNLAWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.ZapierToolkit', 'langchain_community.utilities.zapier.ZapierNLAWrapper']}, 'ZenGuard AI': {'imports': 'from langchain_community.tools.zenguard import ZenGuardTool', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.zenguard.ZenGuardTool']}, 'AI21 Labs': {'imports': 'from langchain_ai21 import ChatAI21', 'category': 'LLM', 'import_list': ['langchain_ai21.ChatAI21']}, 'Alibaba Cloud PAI EAS': {'imports': 'from langchain_community.chat_models import PaiEasChatEndpoint', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.PaiEasChatEndpoint']}, 'Anthropic': {'imports': 'from langchain_anthropic import ChatAnthropic', 'category': 'LLM', 'import_list': ['langchain_anthropic.ChatAnthropic']}, 'Anyscale': {'imports': 'from langchain_community.chat_models import ChatAnyscale', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatAnyscale']}, 'Azure OpenAI': {'imports': 'from langchain_openai import AzureChatOpenAI', 'category': 'LLM', 'import_list': ['langchain_openai.AzureChatOpenAI']}, 'Azure ML Endpoint': {'imports': 'from langchain_community.chat_models.azureml_endpoint import AzureMLChatOnlineEndpoint', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.azureml_endpoint.AzureMLChatOnlineEndpoint']}, 'Baichuan Chat': {'imports': 'from langchain_community.chat_models import ChatBaichuan', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatBaichuan']}, 'Baidu Qianfan': {'imports': 'from langchain_community.chat_models import QianfanChatEndpoint', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.QianfanChatEndpoint']}, 'AWS Bedrock': {'imports': 'from langchain_aws import ChatBedrock', 'category': 'LLM', 'import_list': ['langchain_aws.ChatBedrock']}, 'Cerebras': {'imports': 'from langchain_cerebras import ChatCerebras', 'category': 'LLM', 'import_list': ['langchain_cerebras.ChatCerebras']}, 'Cloudflare Workers AI': {'imports': 'from langchain_community.chat_models.cloudflare_workersai import ChatCloudflareWorkersAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.cloudflare_workersai.ChatCloudflareWorkersAI']}, 'Cohere': {'imports': 'from langchain_cohere import ChatCohere', 'category': 'LLM', 'import_list': ['langchain_cohere.ChatCohere']}, 'Coze Chat': {'imports': 'from langchain_community.chat_models import ChatCoze', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatCoze']}, 'Dappier AI': {'imports': 'from langchain_community.chat_models.dappier import ChatDappierAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.dappier.ChatDappierAI']}, 'Databricks': {'imports': 'from databricks_langchain import ChatDatabricks', 'category': 'LLM', 'import_list': ['databricks_langchain.ChatDatabricks']}, 'DeepInfra': {'imports': 'from langchain_community.chat_models import ChatDeepInfra', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatDeepInfra']}, 'DeepSeek': {'imports': 'from langchain_deepseek import ChatDeepSeek', 'category': 'LLM', 'import_list': ['langchain_deepseek.ChatDeepSeek']}, 'EverlyAI': {'imports': 'from langchain_community.chat_models import ChatEverlyAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatEverlyAI']}, 'Fireworks': {'imports': 'from langchain_fireworks import ChatFireworks', 'category': 'LLM', 'import_list': ['langchain_fireworks.ChatFireworks']}, 'ChatFriendli': {'imports': 'from langchain_community.chat_models.friendli import ChatFriendli', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.friendli.ChatFriendli']}, 'GigaChat': {'imports': 'from langchain_gigachat import GigaChat', 'category': 'LLM', 'import_list': ['langchain_gigachat.GigaChat']}, 'Goodfire': {'imports': 'from langchain_goodfire import ChatGoodfire', 'category': 'LLM', 'import_list': ['langchain_goodfire.ChatGoodfire']}, 'Google AI': {'imports': 'from langchain_google_genai import ChatGoogleGenerativeAI', 'category': 'LLM', 'import_list': ['langchain_google_genai.ChatGoogleGenerativeAI']}, 'Google Cloud Vertex AI': {'imports': 'from langchain_google_vertexai import ChatVertexAI', 'category': 'LLM', 'import_list': ['langchain_google_vertexai.ChatVertexAI']}, 'GPTRouter': {'imports': 'from langchain_community.chat_models import GPTRouter', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.GPTRouter']}, 'Groq': {'imports': 'from langchain_groq import ChatGroq', 'category': 'LLM', 'import_list': ['langchain_groq.ChatGroq']}, 'ChatHuggingFace': {'imports': 'from langchain_huggingface import ChatHuggingFace', 'category': 'LLM', 'import_list': ['langchain_huggingface.ChatHuggingFace']}, 'IBM watsonx.ai': {'imports': 'from langchain_ibm import WatsonxRerank', 'category': 'RETRIEVER', 'import_list': ['langchain_ibm.WatsonxRerank']}, 'JinaChat': {'imports': 'from langchain_community.chat_models import JinaChat', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.JinaChat']}, 'Kinetica': {'imports': 'from langchain_community.document_loaders.kinetica_loader import KineticaLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.kinetica_loader.KineticaLoader']}, 'Konko': {'imports': 'from langchain_community.chat_models import ChatKonko', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatKonko']}, 'LiteLLM': {'imports': 'from langchain_community.chat_models import ChatLiteLLM', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatLiteLLM']}, 'LiteLLMRouter': {'imports': 'from langchain_community.chat_models import ChatLiteLLMRouter', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatLiteLLMRouter']}, 'Llama 2 Chat': {'imports': 'from langchain_experimental.chat_models import Llama2Chat', 'category': 'LLM', 'import_list': ['langchain_experimental.chat_models.Llama2Chat']}, 'Llama API': {'imports': 'from langchain_experimental.llms import ChatLlamaAPI', 'category': 'LLM', 'import_list': ['langchain_experimental.llms.ChatLlamaAPI']}, 'LlamaEdge': {'imports': 'from langchain_community.chat_models.llama_edge import LlamaEdgeChatService', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.llama_edge.LlamaEdgeChatService']}, 'Llama.cpp': {'imports': 'from langchain_community.chat_models import ChatLlamaCpp', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatLlamaCpp']}, 'maritalk': {'imports': 'from langchain_community.chat_models import ChatMaritalk', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatMaritalk']}, 'MiniMax': {'imports': 'from langchain_community.chat_models import MiniMaxChat', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.MiniMaxChat']}, 'MistralAI': {'imports': 'from langchain_mistralai import ChatMistralAI', 'category': 'LLM', 'import_list': ['langchain_mistralai.ChatMistralAI']}, 'MLX': {'imports': 'from langchain_community.chat_models.mlx import ChatMLX', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.mlx.ChatMLX']}, 'ModelScope': {'imports': 'from langchain_modelscope import ModelScopeChatEndpoint', 'category': 'LLM', 'import_list': ['langchain_modelscope.ModelScopeChatEndpoint']}, 'Moonshot': {'imports': 'from langchain_community.chat_models.moonshot import MoonshotChat', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.moonshot.MoonshotChat']}, 'Naver': {'imports': 'from langchain_community.chat_models import ChatClovaX', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatClovaX']}, 'NVIDIA AI Endpoints': {'imports': 'from langchain_nvidia_ai_endpoints import ChatNVIDIA', 'category': 'LLM', 'import_list': ['langchain_nvidia_ai_endpoints.ChatNVIDIA']}, 'ChatOCIModelDeployment': {'imports': 'from langchain_community.chat_models import ChatOCIModelDeployment', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatOCIModelDeployment']}, 'OCIGenAI': {'imports': 'from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.oci_generative_ai.ChatOCIGenAI']}, 'ChatOctoAI': {'imports': 'from langchain_community.chat_models import ChatOctoAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatOctoAI']}, 'Ollama': {'imports': 'from langchain_ollama import ChatOllama', 'category': 'LLM', 'import_list': ['langchain_ollama.ChatOllama']}, 'OpenAI': {'imports': 'from langchain_openai import ChatOpenAI', 'category': 'LLM', 'import_list': ['langchain_openai.ChatOpenAI']}, 'Outlines': {'imports': 'from langchain_community.chat_models.outlines import ChatOutlines', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.outlines.ChatOutlines']}, 'Perplexity': {'imports': 'from langchain_community.chat_models import ChatPerplexity', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatPerplexity']}, 'Pipeshift': {'imports': 'from langchain_pipeshift import ChatPipeshift', 'category': 'LLM', 'import_list': ['langchain_pipeshift.ChatPipeshift']}, 'ChatPredictionGuard': {'imports': 'from langchain_predictionguard import ChatPredictionGuard', 'category': 'LLM', 'import_list': ['langchain_predictionguard.ChatPredictionGuard']}, 'PremAI': {'imports': 'from langchain_community.chat_models import ChatPremAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatPremAI']}, 'PromptLayer ChatOpenAI': {'imports': 'from langchain_community.chat_models import PromptLayerChatOpenAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.PromptLayerChatOpenAI']}, 'Reka': {'imports': 'from langchain_community.chat_models import ChatReka', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatReka']}, 'SambaNovaCloud': {'imports': 'from langchain_sambanova import ChatSambaNovaCloud', 'category': 'LLM', 'import_list': ['langchain_sambanova.ChatSambaNovaCloud']}, 'SambaStudio': {'imports': 'from langchain_sambanova import ChatSambaStudio', 'category': 'LLM', 'import_list': ['langchain_sambanova.ChatSambaStudio']}, 'Snowflake Cortex': {'imports': 'from langchain_community.chat_models import ChatSnowflakeCortex', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatSnowflakeCortex']}, 'solar': {'imports': 'from langchain_community.chat_models.solar import SolarChat', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.solar.SolarChat']}, 'SparkLLM Chat': {'imports': 'from langchain_community.chat_models import ChatSparkLLM', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatSparkLLM']}, 'Nebula (Symbl.ai)': {'imports': 'from langchain_community.chat_models.symblai_nebula import ChatNebula', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.symblai_nebula.ChatNebula']}, 'Tencent Hunyuan': {'imports': 'from langchain_community.chat_models import ChatHunyuan', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatHunyuan']}, 'Together': {'imports': 'from langchain_together import ChatTogether', 'category': 'LLM', 'import_list': ['langchain_together.ChatTogether']}, 'Tongyi Qwen': {'imports': 'from langchain_community.chat_models.tongyi import ChatTongyi', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.tongyi.ChatTongyi']}, 'Upstage': {'imports': 'from langchain_upstage import UpstageDocumentParseLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_upstage.UpstageDocumentParseLoader']}, 'Volc Enging Maas': {'imports': 'from langchain_community.chat_models import VolcEngineMaasChat', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.VolcEngineMaasChat']}, 'Writer': {'imports': 'from langchain_community.chat_models.writer import ChatWriter', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.writer.ChatWriter']}, 'xAI': {'imports': 'from langchain_xai import ChatXAI', 'category': 'LLM', 'import_list': ['langchain_xai.ChatXAI']}, 'YandexGPT': {'imports': 'from langchain_community.chat_models import ChatYandexGPT', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatYandexGPT']}, 'ChatYI': {'imports': 'from langchain_community.chat_models.yi import ChatYi', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.yi.ChatYi']}, 'Yuan2.0': {'imports': 'from langchain_community.chat_models import ChatYuan2', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatYuan2']}, 'ZHIPU AI': {'imports': 'from langchain_community.chat_models import ChatZhipuAI', 'category': 'LLM', 'import_list': ['langchain_community.chat_models.ChatZhipuAI']}, 'Amazon Kendra': {'imports': 'from langchain_community.retrievers import AmazonKendraRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.AmazonKendraRetriever']}, 'Arcee': {'imports': 'from langchain_community.retrievers import ArceeRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.ArceeRetriever']}, 'Arxiv': {'imports': 'from langchain_community.retrievers import ArxivRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.ArxivRetriever']}, 'Azure AI Search': {'imports': 'from langchain_community.vectorstores.azuresearch import AzureSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.azuresearch.AzureSearch']}, 'Bedrock (Knowledge Bases)': {'imports': 'from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_aws.retrievers.AmazonKnowledgeBasesRetriever']}, 'BM25': {'imports': 'from langchain_community.retrievers import BM25Retriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.BM25Retriever']}, 'Box': {'imports': 'from langchain_box.blob_loaders import BoxBlobLoader\\nor\\nfrom langchain_box.document_loaders import BoxLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_box.blob_loaders.BoxBlobLoader', 'langchain_box.document_loaders.BoxLoader']}, 'BREEBS (Open Knowledge)': {'imports': 'from langchain_community.retrievers import BreebsRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.BreebsRetriever']}, 'Chaindesk': {'imports': 'from langchain_community.retrievers import ChaindeskRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.ChaindeskRetriever']}, 'ChatGPT plugin': {'imports': 'from langchain_community.retrievers import ChatGPTPluginRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.ChatGPTPluginRetriever']}, 'Cohere reranker': {'imports': 'from langchain_cohere import CohereRerank', 'category': 'RETRIEVER', 'import_list': ['langchain_cohere.CohereRerank']}, 'Cohere RAG': {'imports': 'from langchain_cohere import CohereRagRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_cohere.CohereRagRetriever']}, 'DocArray': {'imports': 'from langchain_community.retrievers import DocArrayRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.DocArrayRetriever']}, 'Dria': {'imports': 'from langchain_community.retrievers import DriaRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.DriaRetriever']}, 'ElasticSearch BM25': {'imports': 'from langchain_community.retrievers import ElasticSearchBM25Retriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.ElasticSearchBM25Retriever']}, 'ElasticsearchRetriever': {'imports': 'from langchain_elasticsearch import ElasticsearchRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_elasticsearch.ElasticsearchRetriever']}, 'Embedchain': {'imports': 'from langchain_community.retrievers import EmbedchainRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.EmbedchainRetriever']}, 'FlashRank reranker': {'imports': 'from langchain.retrievers.document_compressors import FlashrankRerank', 'category': 'RETRIEVER', 'import_list': ['langchain.retrievers.document_compressors.FlashrankRerank']}, 'Fleet AI Context': {'imports': 'from context import download_embeddings', 'category': 'RETRIEVER', 'import_list': ['context.download_embeddings']}, 'Google Vertex AI Search': {'imports': 'from langchain_google_community import VertexAIMultiTurnSearchRetriever\\nor\\nfrom langchain_google_community import VertexAISearchRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_google_community.VertexAIMultiTurnSearchRetriever', 'langchain_google_community.VertexAISearchRetriever']}, 'JaguarDB Vector Database': {'imports': 'from langchain_community.vectorstores.jaguar import Jaguar', 'category': 'RETRIEVER', 'import_list': ['langchain_community.vectorstores.jaguar.Jaguar']}, 'Kay.ai': {'imports': 'from langchain_community.retrievers import KayAiRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.KayAiRetriever']}, 'Kinetica Vectorstore based Retriever': {'imports': 'from langchain_community.vectorstores import Kinetica', 'category': 'RETRIEVER', 'import_list': ['langchain_community.vectorstores.Kinetica']}, 'kNN': {'imports': 'from langchain_community.retrievers import KNNRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.KNNRetriever']}, 'LinkupSearchRetriever': {'imports': 'from langchain_linkup import LinkupSearchRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_linkup.LinkupSearchRetriever']}, 'LLMLingua Document Compressor': {'imports': 'from langchain_community.document_compressors import LLMLinguaCompressor', 'category': 'RETRIEVER', 'import_list': ['langchain_community.document_compressors.LLMLinguaCompressor']}, 'LOTR (Merger Retriever)': {'imports': 'from langchain.retrievers import MergerRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain.retrievers.MergerRetriever']}, 'Metal': {'imports': 'from langchain_community.retrievers import MetalRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.MetalRetriever']}, 'Milvus Hybrid Search': {'imports': 'from langchain_milvus.retrievers import MilvusCollectionHybridSearchRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_milvus.retrievers.MilvusCollectionHybridSearchRetriever']}, 'NanoPQ (Product Quantization)': {'imports': 'from langchain_community.retrievers import NanoPQRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.NanoPQRetriever']}, 'needle': {'imports': 'from langchain_community.retrievers.needle import NeedleRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.needle.NeedleRetriever']}, 'Nimble': {'imports': 'from langchain_nimble import NimbleSearchRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_nimble.NimbleSearchRetriever']}, 'Outline': {'imports': 'from langchain_community.retrievers import OutlineRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.OutlineRetriever']}, 'Pinecone Hybrid Search': {'imports': 'from langchain_community.retrievers import PineconeHybridSearchRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.PineconeHybridSearchRetriever']}, 'Qdrant Sparse Vector': {'imports': 'from langchain_community.retrievers import QdrantSparseVectorRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.QdrantSparseVectorRetriever']}, 'RAGatouille': {'imports': 'from ragatouille import RAGPretrainedModel', 'category': 'RETRIEVER', 'import_list': ['ragatouille.RAGPretrainedModel']}, 'RePhraseQuery': {'imports': 'from langchain.retrievers import RePhraseQueryRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain.retrievers.RePhraseQueryRetriever']}, 'Rememberizer': {'imports': 'from langchain_community.retrievers import RememberizerRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.RememberizerRetriever']}, 'Self-querying retriever': {'imports': 'from langchain.retrievers.self_query.base import SelfQueryRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain.retrievers.self_query.base.SelfQueryRetriever']}, 'SingleStoreDB': {'imports': 'from langchain_community.vectorstores import SingleStoreDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.SingleStoreDB']}, 'SVM': {'imports': 'from langchain_community.retrievers import SVMRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.SVMRetriever']}, 'TavilySearchAPI': {'imports': 'from langchain_community.retrievers import TavilySearchAPIRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.TavilySearchAPIRetriever']}, 'TF-IDF': {'imports': 'from langchain_community.retrievers import TFIDFRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.TFIDFRetriever']}, 'NeuralDB': {'imports': 'from langchain_community.retrievers import NeuralDBRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.NeuralDBRetriever']}, 'Vespa': {'imports': 'from langchain_community.vectorstores import VespaStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.VespaStore']}, 'You.com': {'imports': 'from langchain_community.retrievers.you import YouRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.you.YouRetriever']}, 'Zep Cloud': {'imports': 'from langchain_community.vectorstores import ZepCloudVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.ZepCloudVectorStore']}, 'Zep Open Source': {'imports': 'from langchain_community.retrievers.zep import ZepRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_community.retrievers.zep.ZepRetriever']}, 'Zilliz Cloud Pipeline': {'imports': 'from langchain_milvus import ZillizCloudPipelineRetriever', 'category': 'RETRIEVER', 'import_list': ['langchain_milvus.ZillizCloudPipelineRetriever']}, 'acreom': {'imports': 'from langchain_community.document_loaders import AcreomLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AcreomLoader']}, 'AirbyteLoader': {'imports': 'from langchain_airbyte import AirbyteLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_airbyte.AirbyteLoader']}, 'Airtable': {'imports': 'from langchain_community.document_loaders import AirtableLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AirtableLoader']}, 'Alibaba Cloud MaxCompute': {'imports': 'from langchain_community.document_loaders import MaxComputeLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.MaxComputeLoader']}, 'Amazon Textract': {'imports': 'from langchain_community.document_loaders import AmazonTextractPDFLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AmazonTextractPDFLoader']}, 'Apify Dataset': {'imports': 'from langchain_community.document_loaders import ApifyDatasetLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ApifyDatasetLoader']}, 'ArcGIS': {'imports': 'from langchain_community.document_loaders import ArcGISLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ArcGISLoader']}, 'ArxivLoader': {'imports': 'from langchain_community.document_loaders import ArxivLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ArxivLoader']}, 'AssemblyAI Audio Transcripts': {'imports': 'from langchain_community.document_loaders import AssemblyAIAudioTranscriptLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AssemblyAIAudioTranscriptLoader']}, 'AstraDB': {'imports': 'from langchain_community.document_loaders import AstraDBLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AstraDBLoader']}, 'Async Chromium': {'imports': 'from langchain_community.document_loaders import AsyncChromiumLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AsyncChromiumLoader']}, 'AsyncHtml': {'imports': 'from langchain_community.document_loaders import AsyncHtmlLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AsyncHtmlLoader']}, 'Athena': {'imports': 'from langchain_community.document_loaders.athena import AthenaLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.athena.AthenaLoader']}, 'AWS S3 Directory': {'imports': 'from langchain_community.document_loaders import S3DirectoryLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.S3DirectoryLoader']}, 'AWS S3 File': {'imports': 'from langchain_community.document_loaders import S3FileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.S3FileLoader']}, 'AZLyrics': {'imports': 'from langchain_community.document_loaders import AZLyricsLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AZLyricsLoader']}, 'Azure AI Data': {'imports': 'from langchain_community.document_loaders import AzureAIDataLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AzureAIDataLoader']}, 'Azure Blob Storage Container': {'imports': 'from langchain_community.document_loaders import AzureBlobStorageContainerLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AzureBlobStorageContainerLoader']}, 'Azure Blob Storage File': {'imports': 'from langchain_community.document_loaders import AzureBlobStorageFileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AzureBlobStorageFileLoader']}, 'Azure AI Document Intelligence': {'imports': 'from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.AzureAIDocumentIntelligenceLoader']}, 'BibTeX': {'imports': 'from langchain_community.document_loaders import BibtexLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.BibtexLoader']}, 'BiliBili': {'imports': 'from langchain_community.document_loaders import BiliBiliLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.BiliBiliLoader']}, 'Blackboard': {'imports': 'from langchain_community.document_loaders import BlackboardLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.BlackboardLoader']}, 'Blockchain': {'imports': 'from langchain_community.document_loaders.blockchain import BlockchainDocumentLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.blockchain.BlockchainDocumentLoader']}, 'Browserbase': {'imports': 'from langchain_community.document_loaders import BrowserbaseLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.BrowserbaseLoader']}, 'Browserless': {'imports': 'from langchain_community.document_loaders import BrowserlessLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.BrowserlessLoader']}, 'BSHTMLLoader': {'imports': 'from langchain_community.document_loaders import BSHTMLLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.BSHTMLLoader']}, 'Cassandra': {'imports': 'from langchain_community.document_loaders import CassandraLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.CassandraLoader']}, 'ChatGPT Data': {'imports': 'from langchain_community.document_loaders.chatgpt import ChatGPTLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.chatgpt.ChatGPTLoader']}, 'College Confidential': {'imports': 'from langchain_community.document_loaders import CollegeConfidentialLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.CollegeConfidentialLoader']}, 'Concurrent Loader': {'imports': 'from langchain_community.document_loaders import ConcurrentLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ConcurrentLoader']}, 'Confluence': {'imports': 'from langchain_community.document_loaders import ConfluenceLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ConfluenceLoader']}, 'CoNLL-U': {'imports': 'from langchain_community.document_loaders import CoNLLULoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.CoNLLULoader']}, 'Couchbase': {'imports': 'from langchain_couchbase.vectorstores import CouchbaseVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_couchbase.vectorstores.CouchbaseVectorStore']}, 'CSV': {'imports': 'from langchain_community.document_loaders.csv_loader import CSVLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.csv_loader.CSVLoader']}, 'Cube Semantic Layer': {'imports': 'from langchain_community.document_loaders import CubeSemanticLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.CubeSemanticLoader']}, 'Datadog Logs': {'imports': 'from langchain_community.document_loaders import DatadogLogsLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.DatadogLogsLoader']}, 'Dedoc': {'imports': 'from langchain_community.document_loaders import DedocFileLoader\\nor\\nfrom langchain_community.document_loaders import DedocPDFLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.DedocFileLoader', 'langchain_community.document_loaders.DedocPDFLoader']}, 'Diffbot': {'imports': 'from langchain_community.document_loaders import DiffbotLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.DiffbotLoader']}, 'Discord': {'imports': 'from langchain_community.document_loaders.discord import DiscordChatLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.discord.DiscordChatLoader']}, 'Docling': {'imports': 'from langchain_docling import DoclingLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_docling.DoclingLoader']}, 'Docugami': {'imports': 'from docugami_langchain.document_loaders import DocugamiLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['docugami_langchain.document_loaders.DocugamiLoader']}, 'Docusaurus': {'imports': 'from langchain_community.document_loaders import DocusaurusLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.DocusaurusLoader']}, 'Dropbox': {'imports': 'from langchain_community.document_loaders import DropboxLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.DropboxLoader']}, 'DuckDB': {'imports': 'from langchain_community.vectorstores import DuckDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.DuckDB']}, 'Email': {'imports': 'from langchain_community.document_loaders import UnstructuredEmailLoader\\nor\\nfrom langchain_community.document_loaders import OutlookMessageLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredEmailLoader', 'langchain_community.document_loaders.OutlookMessageLoader']}, 'EPub': {'imports': 'from langchain_community.document_loaders import UnstructuredEPubLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredEPubLoader']}, 'Etherscan': {'imports': 'from langchain_community.document_loaders import EtherscanLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.EtherscanLoader']}, 'EverNote': {'imports': 'from langchain_community.document_loaders import EverNoteLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.EverNoteLoader']}, 'Facebook Chat': {'imports': 'from langchain_community.document_loaders import FacebookChatLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.FacebookChatLoader']}, 'Fauna': {'imports': 'from langchain_community.document_loaders.fauna import FaunaLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.fauna.FaunaLoader']}, 'Figma': {'imports': 'from langchain_community.document_loaders.figma import FigmaFileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.figma.FigmaFileLoader']}, 'FireCrawl': {'imports': 'from langchain_community.document_loaders.firecrawl import FireCrawlLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.firecrawl.FireCrawlLoader']}, 'Geopandas': {'imports': 'from langchain_community.document_loaders import GeoDataFrameLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.GeoDataFrameLoader']}, 'Git': {'imports': 'from langchain_community.document_loaders import GitLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.GitLoader']}, 'GitBook': {'imports': 'from langchain_community.document_loaders import GitbookLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.GitbookLoader']}, 'GitHub': {'imports': 'from langchain_community.document_loaders import GitHubIssuesLoader\\nor\\nfrom langchain_community.document_loaders import GithubFileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.GitHubIssuesLoader', 'langchain_community.document_loaders.GithubFileLoader']}, 'Glue Catalog': {'imports': 'from langchain_community.document_loaders.glue_catalog import GlueCatalogLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.glue_catalog.GlueCatalogLoader']}, 'Google AlloyDB for PostgreSQL': {'imports': 'from langchain_google_alloydb_pg import AlloyDBVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_google_alloydb_pg.AlloyDBVectorStore']}, 'Google BigQuery': {'imports': 'from langchain_google_community import BigQueryLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_community.BigQueryLoader']}, 'Google Bigtable': {'imports': 'from langchain_google_bigtable import BigtableLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_bigtable.BigtableLoader']}, 'Google Cloud SQL for SQL server': {'imports': 'from langchain_google_cloud_sql_mssql import MSSQLLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_cloud_sql_mssql.MSSQLLoader']}, 'Google Cloud SQL for MySQL': {'imports': 'from langchain_google_cloud_sql_mysql import MySQLVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_google_cloud_sql_mysql.MySQLVectorStore']}, 'Google Cloud SQL for PostgreSQL': {'imports': 'from langchain_google_cloud_sql_pg import PostgresVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_google_cloud_sql_pg.PostgresVectorStore']}, 'Google Cloud Storage Directory': {'imports': 'from langchain_google_community import GCSDirectoryLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_community.GCSDirectoryLoader']}, 'Google Cloud Storage File': {'imports': 'from langchain_google_community import GCSFileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_community.GCSFileLoader']}, 'Google Firestore in Datastore Mode': {'imports': 'from langchain_google_datastore import DatastoreLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_datastore.DatastoreLoader']}, 'Google El Carro for Oracle Workloads': {'imports': 'from langchain_google_el_carro import ElCarroLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_el_carro.ElCarroLoader']}, 'Google Firestore (Native Mode)': {'imports': 'from langchain_google_firestore import FirestoreLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_firestore.FirestoreLoader']}, 'Google Memorystore for Redis': {'imports': 'from langchain_google_memorystore_redis import RedisVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_google_memorystore_redis.RedisVectorStore']}, 'Google Spanner': {'imports': 'from langchain_google_spanner import SpannerVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_google_spanner.SpannerVectorStore']}, 'Google Speech-to-Text Audio Transcripts': {'imports': 'from langchain_google_community import SpeechToTextLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_google_community.SpeechToTextLoader']}, 'Grobid': {'imports': 'from langchain_community.document_loaders.generic import GenericLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.generic.GenericLoader']}, 'Gutenberg': {'imports': 'from langchain_community.document_loaders import GutenbergLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.GutenbergLoader']}, 'Hacker News': {'imports': 'from langchain_community.document_loaders import HNLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.HNLoader']}, 'Huawei OBS Directory': {'imports': 'from langchain_community.document_loaders import OBSDirectoryLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.OBSDirectoryLoader']}, 'Huawei OBS File': {'imports': 'from langchain_community.document_loaders.obs_file import OBSFileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.obs_file.OBSFileLoader']}, 'HuggingFace dataset': {'imports': 'from langchain_community.document_loaders import HuggingFaceDatasetLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.HuggingFaceDatasetLoader']}, 'HyperbrowserLoader': {'imports': 'from langchain_hyperbrowser import HyperbrowserLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_hyperbrowser.HyperbrowserLoader']}, 'iFixit': {'imports': 'from langchain_community.document_loaders import IFixitLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.IFixitLoader']}, 'Images': {'imports': 'from langchain_community.document_loaders.image import UnstructuredImageLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.image.UnstructuredImageLoader']}, 'Image captions': {'imports': 'from langchain_community.document_loaders import ImageCaptionLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ImageCaptionLoader']}, 'IMSDb': {'imports': 'from langchain_community.document_loaders import IMSDbLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.IMSDbLoader']}, 'Iugu': {'imports': 'from langchain_community.document_loaders import IuguLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.IuguLoader']}, 'Joplin': {'imports': 'from langchain_community.document_loaders import JoplinLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.JoplinLoader']}, 'JSONLoader': {'imports': 'from langchain_community.document_loaders import JSONLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.JSONLoader']}, 'Jupyter Notebook': {'imports': 'from langchain_community.document_loaders import NotebookLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.NotebookLoader']}, 'lakeFS': {'imports': 'from langchain_community.document_loaders import LakeFSLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.LakeFSLoader']}, 'LangSmithLoader': {'imports': 'from langchain_core.document_loaders import LangSmithLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_core.document_loaders.LangSmithLoader']}, 'LarkSuite (FeiShu)': {'imports': 'from langchain_community.document_loaders.larksuite import LarkSuiteDocLoader\\nor\\nfrom langchain_community.document_loaders.larksuite import LarkSuiteWikiLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.larksuite.LarkSuiteDocLoader', 'langchain_community.document_loaders.larksuite.LarkSuiteWikiLoader']}, 'LLM Sherpa': {'imports': 'from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.llmsherpa.LLMSherpaFileLoader']}, 'Mastodon': {'imports': 'from langchain_community.document_loaders import MastodonTootsLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.MastodonTootsLoader']}, 'MathPixPDFLoader': {'imports': 'from langchain_community.document_loaders import MathpixPDFLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.MathpixPDFLoader']}, 'MediaWiki Dump': {'imports': 'from langchain_community.document_loaders import MWDumpLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.MWDumpLoader']}, 'Merge Documents Loader': {'imports': 'from langchain_community.document_loaders.merge import MergedDataLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.merge.MergedDataLoader']}, 'mhtml': {'imports': 'from langchain_community.document_loaders import MHTMLLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.MHTMLLoader']}, 'Microsoft Excel': {'imports': 'from langchain_community.document_loaders import UnstructuredExcelLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredExcelLoader']}, 'Microsoft OneDrive': {'imports': 'from langchain_community.document_loaders.onedrive import OneDriveLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.onedrive.OneDriveLoader']}, 'Microsoft OneNote': {'imports': 'from langchain_community.document_loaders.onenote import OneNoteLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.onenote.OneNoteLoader']}, 'Microsoft PowerPoint': {'imports': 'from langchain_community.document_loaders import UnstructuredPowerPointLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredPowerPointLoader']}, 'Microsoft SharePoint': {'imports': 'from langchain_community.document_loaders.sharepoint import SharePointLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.sharepoint.SharePointLoader']}, 'Microsoft Word': {'imports': 'from langchain_community.document_loaders import UnstructuredWordDocumentLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredWordDocumentLoader']}, 'Near Blockchain': {'imports': 'from MintbaseLoader import MintbaseDocumentLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['MintbaseLoader.MintbaseDocumentLoader']}, 'Modern Treasury': {'imports': 'from langchain_community.document_loaders import ModernTreasuryLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ModernTreasuryLoader']}, 'MongoDB': {'imports': 'from langchain_community.document_loaders.mongodb import MongodbLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.mongodb.MongodbLoader']}, 'Needle Document Loader': {'imports': 'from langchain_community.document_loaders.needle import NeedleLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.needle.NeedleLoader']}, 'News URL': {'imports': 'from langchain_community.document_loaders import NewsURLLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.NewsURLLoader']}, 'Notion DB 2/2': {'imports': 'from langchain_community.document_loaders import NotionDBLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.NotionDBLoader']}, 'Obsidian': {'imports': 'from langchain_community.document_loaders import ObsidianLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ObsidianLoader']}, 'Open Document Format (ODT)': {'imports': 'from langchain_community.document_loaders import UnstructuredODTLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredODTLoader']}, 'Open City Data': {'imports': 'from langchain_community.document_loaders import OpenCityDataLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.OpenCityDataLoader']}, 'Oracle Autonomous Database': {'imports': 'from langchain_community.document_loaders import OracleAutonomousDatabaseLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.OracleAutonomousDatabaseLoader']}, 'Oracle AI Vector Search: Document Processing': {'imports': 'from langchain_community.document_loaders.oracleai import OracleDocLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.oracleai.OracleDocLoader']}, 'Org-mode': {'imports': 'from langchain_community.document_loaders import UnstructuredOrgModeLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredOrgModeLoader']}, 'Pandas DataFrame': {'imports': 'from langchain_community.document_loaders import DataFrameLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.DataFrameLoader']}, 'PDFMinerLoader': {'imports': 'from langchain_community.document_loaders import PDFMinerLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PDFMinerLoader']}, 'PDFPlumber': {'imports': 'from langchain_community.document_loaders import PDFPlumberLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PDFPlumberLoader']}, 'Pebblo Safe DocumentLoader': {'imports': 'from langchain_community.document_loaders import PebbloSafeLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PebbloSafeLoader']}, 'Polars DataFrame': {'imports': 'from langchain_community.document_loaders import PolarsDataFrameLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PolarsDataFrameLoader']}, 'Psychic': {'imports': 'from langchain_community.document_loaders import PsychicLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PsychicLoader']}, 'PullMdLoader': {'imports': 'from langchain_pull_md.markdown_loader import PullMdLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_pull_md.markdown_loader.PullMdLoader']}, 'PyMuPDFLoader': {'imports': 'from langchain_community.document_loaders import PyMuPDFLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PyMuPDFLoader']}, 'PyPDFDirectoryLoader': {'imports': 'from langchain_community.document_loaders import PyPDFDirectoryLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PyPDFDirectoryLoader']}, 'PyPDFium2Loader': {'imports': 'from langchain_community.document_loaders import PyPDFium2Loader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PyPDFium2Loader']}, 'PyPDFLoader': {'imports': 'from langchain_community.document_loaders import PyPDFLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PyPDFLoader']}, 'PySpark': {'imports': 'from langchain_community.document_loaders import PySparkDataFrameLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.PySparkDataFrameLoader']}, 'Quip': {'imports': 'from langchain_community.document_loaders.quip import QuipLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.quip.QuipLoader']}, 'ReadTheDocs Documentation': {'imports': 'from langchain_community.document_loaders import ReadTheDocsLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ReadTheDocsLoader']}, 'Recursive URL': {'imports': 'from langchain_community.document_loaders import RecursiveUrlLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.RecursiveUrlLoader']}, 'Reddit': {'imports': 'from langchain_community.document_loaders import RedditPostsLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.RedditPostsLoader']}, 'Roam': {'imports': 'from langchain_community.document_loaders import RoamLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.RoamLoader']}, 'Rockset': {'imports': 'from langchain_community.vectorstores import Rockset', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Rockset']}, 'rspace': {'imports': 'from langchain_community.document_loaders.rspace import RSpaceLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.rspace.RSpaceLoader']}, 'RSS Feeds': {'imports': 'from langchain_community.document_loaders import RSSFeedLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.RSSFeedLoader']}, 'RST': {'imports': 'from langchain_community.document_loaders import UnstructuredRSTLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredRSTLoader']}, 'scrapfly': {'imports': 'from langchain_community.document_loaders import ScrapflyLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ScrapflyLoader']}, 'ScrapingAnt': {'imports': 'from langchain_community.document_loaders import ScrapingAntLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ScrapingAntLoader']}, 'Sitemap': {'imports': 'from langchain_community.document_loaders.sitemap import SitemapLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.sitemap.SitemapLoader']}, 'Slack': {'imports': 'from langchain_community.document_loaders import SlackDirectoryLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.SlackDirectoryLoader']}, 'Snowflake': {'imports': 'from langchain_community.document_loaders import SnowflakeLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.SnowflakeLoader']}, 'Spider': {'imports': 'from langchain_community.document_loaders import SpiderLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.SpiderLoader']}, 'Spreedly': {'imports': 'from langchain_community.document_loaders import SpreedlyLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.SpreedlyLoader']}, 'Subtitle': {'imports': 'from langchain_community.document_loaders import SRTLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.SRTLoader']}, 'SurrealDB': {'imports': 'from langchain_community.vectorstores import SurrealDBStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.SurrealDBStore']}, 'Telegram': {'imports': 'from langchain_community.document_loaders import TelegramChatApiLoader\\nor\\nfrom langchain_community.document_loaders import TelegramChatFileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.TelegramChatApiLoader', 'langchain_community.document_loaders.TelegramChatFileLoader']}, 'Tencent COS Directory': {'imports': 'from langchain_community.document_loaders import TencentCOSDirectoryLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.TencentCOSDirectoryLoader']}, 'Tencent COS File': {'imports': 'from langchain_community.document_loaders import TencentCOSFileLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.TencentCOSFileLoader']}, 'TensorFlow Datasets': {'imports': 'from langchain_community.document_loaders import TensorflowDatasetLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.TensorflowDatasetLoader']}, 'TiDB': {'imports': 'from langchain_community.document_loaders import TiDBLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.TiDBLoader']}, '2Markdown': {'imports': 'from langchain_community.document_loaders import ToMarkdownLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.ToMarkdownLoader']}, 'TOML': {'imports': 'from langchain_community.document_loaders import TomlLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.TomlLoader']}, 'Trello': {'imports': 'from langchain_community.document_loaders import TrelloLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.TrelloLoader']}, 'TSV': {'imports': 'from langchain_community.document_loaders.tsv import UnstructuredTSVLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.tsv.UnstructuredTSVLoader']}, 'Twitter': {'imports': 'from langchain_community.document_loaders import TwitterTweetLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.TwitterTweetLoader']}, 'Unstructured': {'imports': 'from langchain_unstructured import UnstructuredLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_unstructured.UnstructuredLoader']}, 'UnstructuredMarkdownLoader': {'imports': 'from langchain_community.document_loaders import UnstructuredMarkdownLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredMarkdownLoader']}, 'UnstructuredPDFLoader': {'imports': 'from langchain_community.document_loaders import UnstructuredPDFLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredPDFLoader']}, 'URL': {'imports': 'from langchain_community.document_loaders import UnstructuredURLLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredURLLoader']}, 'Vsdx': {'imports': 'from langchain_community.document_loaders import VsdxLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.VsdxLoader']}, 'Weather': {'imports': 'from langchain_community.document_loaders import WeatherDataLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.WeatherDataLoader']}, 'WebBaseLoader': {'imports': 'from langchain_community.document_loaders import WebBaseLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.WebBaseLoader']}, 'WhatsApp Chat': {'imports': 'from langchain_community.document_loaders import WhatsAppChatLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.WhatsAppChatLoader']}, 'UnstructuredXMLLoader': {'imports': 'from langchain_community.document_loaders import UnstructuredXMLLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.UnstructuredXMLLoader']}, 'Xorbits Pandas DataFrame': {'imports': 'from langchain_community.document_loaders import XorbitsLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.XorbitsLoader']}, 'YouTube audio': {'imports': 'from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.blob_loaders.youtube_audio.YoutubeAudioLoader']}, 'YouTube transcripts': {'imports': 'from langchain_community.document_loaders import YoutubeLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.YoutubeLoader']}, 'YoutubeLoaderDL': {'imports': 'from langchain_yt_dlp.youtube_loader import YoutubeLoaderDL', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_yt_dlp.youtube_loader.YoutubeLoaderDL']}, 'Yuque': {'imports': 'from langchain_community.document_loaders import YuqueLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.YuqueLoader']}, 'ZeroxPDFLoader': {'imports': 'from langchain_community.document_loaders.pdf import ZeroxPDFLoader', 'category': 'DOCUMENT_LOADER', 'import_list': ['langchain_community.document_loaders.pdf.ZeroxPDFLoader']}, 'Activeloop Deep Lake': {'imports': 'from langchain_community.vectorstores import DeepLake', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.DeepLake']}, 'Aerospike': {'imports': 'from langchain_community.vectorstores import Aerospike', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Aerospike']}, 'Alibaba Cloud OpenSearch': {'imports': 'from langchain_community.vectorstores import AlibabaCloudOpenSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.AlibabaCloudOpenSearch']}, 'AnalyticDB': {'imports': 'from langchain_community.vectorstores import AnalyticDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.AnalyticDB']}, 'Annoy': {'imports': 'from langchain_community.vectorstores import Annoy', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Annoy']}, 'Apache Doris': {'imports': 'from langchain_community.vectorstores.apache_doris import ApacheDoris', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.apache_doris.ApacheDoris']}, 'ApertureDB': {'imports': 'from langchain_community.vectorstores import ApertureDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.ApertureDB']}, 'Astra DB Vector Store': {'imports': 'from langchain_astradb import AstraDBVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_astradb.AstraDBVectorStore']}, 'Atlas': {'imports': 'from langchain_community.vectorstores import AtlasDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.AtlasDB']}, 'AwaDB': {'imports': 'from langchain_community.vectorstores import AwaDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.AwaDB']}, 'Azure Cosmos DB Mongo vCore': {'imports': 'from langchain_community.vectorstores.azure_cosmos_db import AzureCosmosDBVectorSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.azure_cosmos_db.AzureCosmosDBVectorSearch']}, 'Azure Cosmos DB No SQL': {'imports': 'from langchain_community.vectorstores.azure_cosmos_db_no_sql import AzureCosmosDBNoSqlVectorSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.azure_cosmos_db_no_sql.AzureCosmosDBNoSqlVectorSearch']}, 'Bagel': {'imports': 'from langchain_community.vectorstores import Bagel', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Bagel']}, 'BagelDB': {'imports': 'from langchain_community.vectorstores import Bagel', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Bagel']}, 'Baidu Cloud ElasticSearch VectorSearch': {'imports': 'from langchain_community.vectorstores import BESVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.BESVectorStore']}, 'Apache Cassandra': {'imports': 'from langchain_community.vectorstores import Cassandra', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Cassandra']}, 'Chroma': {'imports': 'from langchain_chroma import Chroma', 'category': 'VECTOR_STORE', 'import_list': ['langchain_chroma.Chroma']}, 'Clarifai': {'imports': 'from langchain_community.vectorstores import Clarifai', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Clarifai']}, 'ClickHouse': {'imports': 'from langchain_community.vectorstores import Clickhouse', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Clickhouse']}, 'DashVector': {'imports': 'from langchain_community.vectorstores import DashVector', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.DashVector']}, 'DatabricksVectorSearch': {'imports': 'from databricks_langchain import DatabricksVectorSearch', 'category': 'VECTOR_STORE', 'import_list': ['databricks_langchain.DatabricksVectorSearch']}, 'DingoDB': {'imports': 'from langchain_community.vectorstores import Dingo', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Dingo']}, 'DocArray HnswSearch': {'imports': 'from langchain_community.vectorstores import DocArrayHnswSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.DocArrayHnswSearch']}, 'DocArray InMemorySearch': {'imports': 'from langchain_community.vectorstores import DocArrayInMemorySearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.DocArrayInMemorySearch']}, 'Amazon Document DB': {'imports': 'from langchain.vectorstores.documentdb import DocumentDBVectorSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain.vectorstores.documentdb.DocumentDBVectorSearch']}, 'China Mobile ECloud ElasticSearch VectorSearch': {'imports': 'from langchain_community.vectorstores import EcloudESVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.EcloudESVectorStore']}, 'Elasticsearch': {'imports': 'from langchain_elasticsearch import ElasticsearchStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_elasticsearch.ElasticsearchStore']}, 'Epsilla': {'imports': 'from langchain_community.vectorstores import Epsilla', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Epsilla']}, 'Faiss': {'imports': 'from langchain_community.vectorstores import FAISS', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.FAISS']}, 'FalkorDBVectorStore': {'imports': 'from langchain_community.vectorstores.falkordb_vector import FalkorDBVector', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.falkordb_vector.FalkorDBVector']}, 'Google BigQuery Vector Search': {'imports': 'from langchain_google_community import BigQueryVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_google_community.BigQueryVectorStore']}, 'Google Firestore': {'imports': 'from langchain_google_firestore import FirestoreVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_google_firestore.FirestoreVectorStore']}, 'Google Vertex AI Feature Store': {'imports': 'from langchain_google_community import VertexFSVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_google_community.VertexFSVectorStore']}, 'Hippo': {'imports': 'from langchain_community.vectorstores.hippo import Hippo', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.hippo.Hippo']}, 'Hologres': {'imports': 'from langchain_community.vectorstores import Hologres', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Hologres']}, 'Infinispan': {'imports': 'from langchain_community.vectorstores import InfinispanVS', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.InfinispanVS']}, 'Jaguar Vector Database': {'imports': 'from langchain_community.vectorstores.jaguar import Jaguar', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.jaguar.Jaguar']}, 'KDB.AI': {'imports': 'from langchain_community.vectorstores import KDBAI', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.KDBAI']}, 'Kinetica Vectorstore API': {'imports': 'from langchain_community.vectorstores import Kinetica', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Kinetica']}, 'LanceDB': {'imports': 'from langchain_community.vectorstores import LanceDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.LanceDB']}, 'Lantern': {'imports': 'from langchain_community.vectorstores import Lantern', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Lantern']}, 'LindormVectorStore': {'imports': 'from langchain_lindorm_integration.vectorstores import LindormVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_lindorm_integration.vectorstores.LindormVectorStore']}, 'LLMRails': {'imports': 'from langchain_community.vectorstores import LLMRails', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.LLMRails']}, 'ManticoreSearch VectorStore': {'imports': 'from langchain_community.vectorstores import ManticoreSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.ManticoreSearch']}, 'Marqo': {'imports': 'from langchain_community.vectorstores import Marqo', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Marqo']}, 'Meilisearch': {'imports': 'from langchain_community.vectorstores import Meilisearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Meilisearch']}, 'Amazon MemoryDB': {'imports': 'from langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_aws.vectorstores.inmemorydb.InMemoryVectorStore']}, 'Milvus': {'imports': 'from langchain_milvus import Milvus', 'category': 'VECTOR_STORE', 'import_list': ['langchain_milvus.Milvus']}, 'Momento Vector Index': {'imports': 'from langchain_community.vectorstores import MomentoVectorIndex', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.MomentoVectorIndex']}, 'MongoDB Atlas': {'imports': 'from langchain_mongodb import MongoDBAtlasVectorSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_mongodb.MongoDBAtlasVectorSearch']}, 'MyScale': {'imports': 'from langchain_community.vectorstores import MyScale', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.MyScale']}, 'Neo4j Vector Index': {'imports': 'from langchain_neo4j import Neo4jVector', 'category': 'VECTOR_STORE', 'import_list': ['langchain_neo4j.Neo4jVector']}, 'NucliaDB': {'imports': 'from langchain_community.vectorstores.nucliadb import NucliaDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.nucliadb.NucliaDB']}, 'OceanbaseVectorStore': {'imports': 'from langchain_oceanbase.vectorstores import OceanbaseVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_oceanbase.vectorstores.OceanbaseVectorStore']}, 'OpenSearch': {'imports': 'from langchain_community.vectorstores import OpenSearchVectorSearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.OpenSearchVectorSearch']}, 'Oracle AI Vector Search: Vector Store': {'imports': 'from langchain_community.vectorstores.oraclevs import OracleVS', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.oraclevs.OracleVS']}, 'Pathway': {'imports': 'from langchain_community.vectorstores import PathwayVectorClient', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.PathwayVectorClient']}, 'Postgres Embedding': {'imports': 'from langchain_community.vectorstores import PGEmbedding', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.PGEmbedding']}, 'PGVecto.rs': {'imports': 'from langchain_community.vectorstores.pgvecto_rs import PGVecto_rs', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.pgvecto_rs.PGVecto_rs']}, 'PGVector': {'imports': 'from langchain_postgres.vectorstores import PGVector', 'category': 'VECTOR_STORE', 'import_list': ['langchain_postgres.vectorstores.PGVector']}, 'Pinecone': {'imports': 'from langchain_pinecone import PineconeVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_pinecone.PineconeVectorStore']}, 'Qdrant': {'imports': 'from langchain_qdrant import QdrantVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_qdrant.QdrantVectorStore']}, 'Redis Vector Store': {'imports': 'from langchain_redis import RedisVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_redis.RedisVectorStore']}, 'Relyt': {'imports': 'from langchain_community.vectorstores import Relyt', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Relyt']}, 'SAP HANA Cloud Vector Engine': {'imports': 'from langchain_community.vectorstores.hanavector import HanaDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.hanavector.HanaDB']}, 'ScaNN': {'imports': 'from langchain_community.vectorstores import ScaNN', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.ScaNN']}, 'SemaDB': {'imports': 'from langchain_community.vectorstores import SemaDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.SemaDB']}, 'scikit-learn': {'imports': 'from langchain_community.vectorstores import SKLearnVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.SKLearnVectorStore']}, 'SQLiteVec': {'imports': 'from langchain_community.vectorstores import SQLiteVec', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.SQLiteVec']}, 'SQLite-VSS': {'imports': 'from langchain_community.vectorstores import SQLiteVSS', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.SQLiteVSS']}, 'SQLServer': {'imports': 'from langchain_sqlserver import SQLServer_VectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_sqlserver.SQLServer_VectorStore']}, 'StarRocks': {'imports': 'from langchain_community.vectorstores import StarRocks', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.StarRocks']}, 'Supabase (Postgres)': {'imports': 'from langchain_community.vectorstores import SupabaseVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.SupabaseVectorStore']}, 'Tablestore': {'imports': 'from langchain_community.vectorstores import TablestoreVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.TablestoreVectorStore']}, 'Tair': {'imports': 'from langchain_community.vectorstores import Tair', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Tair']}, 'Tencent Cloud VectorDB': {'imports': 'from langchain_community.vectorstores import TencentVectorDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.TencentVectorDB']}, 'ThirdAI NeuralDB': {'imports': 'from langchain_community.vectorstores import NeuralDBVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.NeuralDBVectorStore']}, 'TiDB Vector': {'imports': 'from langchain_community.vectorstores import TiDBVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.TiDBVectorStore']}, 'Tigris': {'imports': 'from langchain_community.vectorstores import Tigris', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Tigris']}, 'TileDB': {'imports': 'from langchain_community.vectorstores import TileDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.TileDB']}, 'Timescale Vector (Postgres)': {'imports': 'from langchain_community.vectorstores.timescalevector import TimescaleVector', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.timescalevector.TimescaleVector']}, 'Typesense': {'imports': 'from langchain_community.vectorstores import Typesense', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Typesense']}, 'Upstash Vector': {'imports': 'from langchain_community.vectorstores.upstash import UpstashVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.upstash.UpstashVectorStore']}, 'USearch': {'imports': 'from langchain_community.vectorstores import USearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.USearch']}, 'Vald': {'imports': 'from langchain_community.vectorstores import Vald', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Vald']}, \"Intel's Visual Data Management System (VDMS)\": {'imports': 'from langchain_community.vectorstores import VDMS\\nor\\nfrom langchain_community.vectorstores.vdms import VDMS_Client', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.VDMS', 'langchain_community.vectorstores.vdms.VDMS_Client']}, 'Vearch': {'imports': 'from langchain_community.vectorstores.vearch import Vearch', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.vearch.Vearch']}, 'Vectara': {'imports': 'from langchain_community.vectorstores import Vectara', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Vectara']}, 'viking DB': {'imports': 'from langchain_community.vectorstores.vikingdb import VikingDB', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.vikingdb.VikingDB']}, 'vlite': {'imports': 'from langchain_community.vectorstores import VLite', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.VLite']}, 'Weaviate': {'imports': 'from langchain_weaviate.vectorstores import WeaviateVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_weaviate.vectorstores.WeaviateVectorStore']}, 'Xata': {'imports': 'from langchain_community.vectorstores.xata import XataVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.xata.XataVectorStore']}, 'Yellowbrick': {'imports': 'from langchain_community.vectorstores import Yellowbrick', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Yellowbrick']}, 'Zep': {'imports': 'from langchain_community.vectorstores import ZepVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.ZepVectorStore']}, 'Zilliz': {'imports': 'from langchain_community.vectorstores import Milvus', 'category': 'VECTOR_STORE', 'import_list': ['langchain_community.vectorstores.Milvus']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_imports = parse_all_imports_from_directory(root_directory)\n",
    "\n",
    "possible_tools = []\n",
    "\n",
    "for name, values in predefined_tools_dict.items():\n",
    "\n",
    "    if any(single_import in found_imports for single_import in values[\"import_list\"]):\n",
    "        new_possible_tool = {\"name\": name}\n",
    "        for value_name, value_content in values.items():\n",
    "            new_possible_tool[value_name] = value_content\n",
    "        possible_tools.append(new_possible_tool)\n",
    "    \n",
    "custom_tools = parse_all_custom_tools_from_directory(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Jira Toolkit', 'imports': 'from langchain_community.agent_toolkits.jira.toolkit import JiraToolkit\\nor\\nfrom langchain_community.utilities.jira import JiraAPIWrapper\\n', 'category': 'GENERAL', 'import_list': ['langchain_community.agent_toolkits.jira.toolkit.JiraToolkit', 'langchain_community.utilities.jira.JiraAPIWrapper']}, {'name': 'Tavily Search', 'imports': 'from langchain_community.tools import TavilySearchResults', 'category': 'GENERAL', 'import_list': ['langchain_community.tools.TavilySearchResults']}, {'name': 'Azure OpenAI', 'imports': 'from langchain_openai import AzureChatOpenAI', 'category': 'LLM', 'import_list': ['langchain_openai.AzureChatOpenAI']}, {'name': 'AWS Bedrock', 'imports': 'from langchain_aws import ChatBedrock', 'category': 'LLM', 'import_list': ['langchain_aws.ChatBedrock']}, {'name': 'Pinecone', 'imports': 'from langchain_pinecone import PineconeVectorStore', 'category': 'VECTOR_STORE', 'import_list': ['langchain_pinecone.PineconeVectorStore']}]\n"
     ]
    }
   ],
   "source": [
    "print(possible_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'generate_document_parsed_input_json_tool', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}, {'name': 'retriever_tool', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}, {'name': 'run_query', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}, {'name': 'search_issue', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}, {'name': 'create_issue', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}]\n"
     ]
    }
   ],
   "source": [
    "print(custom_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "def find_package_root(file_path):\n",
    "    \"\"\"\n",
    "    Given a file path, search upward for the highest directory that contains\n",
    "    an __init__.py file. If no __init__.py is found in the file's directory,\n",
    "    returns None.\n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    package_root = None\n",
    "    while True:\n",
    "        if os.path.exists(os.path.join(directory, '__init__.py')):\n",
    "            package_root = directory  # update to current package dir\n",
    "            parent_directory = os.path.dirname(directory)\n",
    "            if parent_directory == directory:\n",
    "                break  # reached filesystem root\n",
    "            directory = parent_directory\n",
    "        else:\n",
    "            break\n",
    "    return package_root\n",
    "\n",
    "def module_name_from_filepath(file_path, package_root):\n",
    "    \"\"\"\n",
    "    Convert a file path into a module name relative to the package root.\n",
    "    For example, if package_root is /path/to/my_package and\n",
    "    file_path is /path/to/my_package/sub/module.py, this returns \"sub.module\".\n",
    "    \n",
    "    If no package_root is found (i.e. file isn't in a package), fall back to\n",
    "    using the filename (without extension) as the module name.\n",
    "    \"\"\"\n",
    "    if package_root:\n",
    "        rel_path = os.path.relpath(file_path, package_root)\n",
    "        mod_name, _ = os.path.splitext(rel_path)\n",
    "        return mod_name.replace(os.path.sep, '.')\n",
    "    else:\n",
    "        return os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "class ToolVisitor(ast.NodeVisitor):\n",
    "    def __init__(self, module_name, file_path):\n",
    "        self.module_name = module_name\n",
    "        self.file_path = file_path\n",
    "        self.scope_stack = []  # To keep track of enclosing class names.\n",
    "        self.tools = []\n",
    "\n",
    "    def visit_ClassDef(self, node):\n",
    "        # Entering a class: push its name onto the scope stack.\n",
    "        self.scope_stack.append(node.name)\n",
    "        self.generic_visit(node)\n",
    "        self.scope_stack.pop()\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        decorator_names = []\n",
    "        for decorator in node.decorator_list:\n",
    "            if isinstance(decorator, ast.Name):\n",
    "                decorator_names.append(decorator.id)\n",
    "            elif isinstance(decorator, ast.Call):\n",
    "                if isinstance(decorator.func, ast.Name):\n",
    "                    decorator_names.append(decorator.func.id)\n",
    "                elif isinstance(decorator.func, ast.Attribute):\n",
    "                    decorator_names.append(decorator.func.attr)\n",
    "\n",
    "        if \"tool\" in decorator_names:\n",
    "            # Build a fully qualified name: module + any class scopes + function name.\n",
    "            full_name_parts = [self.module_name] if self.module_name else []\n",
    "            full_name_parts.extend(self.scope_stack)\n",
    "            full_name_parts.append(node.name)\n",
    "            fully_qualified_name = \".\".join(full_name_parts)\n",
    "            self.tools.append({\n",
    "                \"name\": node.name,\n",
    "                \"fully_qualified_name\": fully_qualified_name,\n",
    "                \"filepath\": self.file_path\n",
    "            })\n",
    "\n",
    "        self.generic_visit(node)\n",
    "\n",
    "def extract_custom_tools_with_ast(file_content, file_path):\n",
    "    \"\"\"\n",
    "    Parse the file content, automatically determine the package root,\n",
    "    and extract all functions decorated with @tool along with their\n",
    "    fully qualified names.\n",
    "    \"\"\"\n",
    "    tree = ast.parse(file_content)\n",
    "    package_root = find_package_root(file_path)\n",
    "    module_name = module_name_from_filepath(file_path, package_root)\n",
    "    visitor = ToolVisitor(module_name, file_path)\n",
    "    visitor.visit(tree)\n",
    "    return visitor.tools\n",
    "\n",
    "def parse_all_custom_tools_from_directory(directory_path):\n",
    "    all_custom_tools = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    file_content = f.read()\n",
    "                    custom_tools = extract_custom_tools_with_ast(file_content, file_path)\n",
    "                    all_custom_tools.extend(custom_tools)\n",
    "    return all_custom_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'generate_document_parsed_input_json_tool', 'fully_qualified_name': 'Tools.create_tools.generate_document_parsed_input_json_tool', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}, {'name': 'retriever_tool', 'fully_qualified_name': 'Tools.create_tools.retriever_tool', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}, {'name': 'run_query', 'fully_qualified_name': 'Tools.create_tools.run_query', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}, {'name': 'search_issue', 'fully_qualified_name': 'Tools.create_tools.search_issue', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}, {'name': 'create_issue', 'fully_qualified_name': 'Tools.create_tools.create_issue', 'filepath': './Gladiator2\\\\Tools\\\\create_tools.py'}]\n"
     ]
    }
   ],
   "source": [
    "custom_tools = parse_all_custom_tools_from_directory(root_directory)\n",
    "print(custom_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all_graph_instances_in_directory(root_directory, graph_class_fqcn, add_conditional_edges_method_name, add_node_method_name):\n",
    "\n",
    "    results = walk_directory_and_parse(root_directory, graph_class_fqcn, add_conditional_edges_method_name, add_node_method_name)\n",
    "    graphs = []\n",
    "    for graph, call_records in results.items():\n",
    "        nodes = []\n",
    "        basic_edges = []\n",
    "        conditional_edges = []\n",
    "\n",
    "        if call_records.get(\"add_node\", False):\n",
    "            call_data = call_records.get(\"add_node\")\n",
    "            for single_call in call_data:\n",
    "                # The actual name if the method received 2 arguments, and the name of the function if it received one\n",
    "                node_name = single_call[\"positional\"][0]\n",
    "                node_definition = single_call[\"node_definition_argument_info\"]\n",
    "                nodes.append({\n",
    "                    \"name\": node_name,\n",
    "                    \"definition\": node_definition\n",
    "                })\n",
    "            \n",
    "        if call_records.get(\"add_edge\", False):\n",
    "            all_node_names = [node[\"name\"] for node in nodes] + [\"START\", \"END\"]\n",
    "            print(all_node_names)\n",
    "            call_data = call_records.get(\"add_edge\")\n",
    "            for single_call in call_data:\n",
    "                nodes_in_edge = []\n",
    "                # Since the call always contains two node names, we just add names from positional and keyword arguments (in this exact order)\n",
    "                for node_name in single_call[\"positional\"]:\n",
    "                    nodes_in_edge.append(node_name)\n",
    "                for _, node_name in single_call[\"keyword\"].items():\n",
    "                    nodes_in_edge.append(node_name)\n",
    "\n",
    "                if nodes_in_edge[0] in all_node_names and nodes_in_edge[1] in all_node_names:\n",
    "                    basic_edges.append({\n",
    "                        \"start_node\": nodes_in_edge[0],\n",
    "                        \"end_node\": nodes_in_edge[1]\n",
    "                    })\n",
    "\n",
    "        if call_records.get(\"add_conditional_edges\", False):\n",
    "            all_node_names = [node[\"name\"] for node in nodes] + [\"START\", \"END\"]\n",
    "            call_data = call_records.get(\"add_conditional_edges\")\n",
    "            for single_call in call_data:\n",
    "                # Resolving arguments to find the source node\n",
    "                arguments = []\n",
    "                for argument in single_call[\"positional\"]:\n",
    "                    arguments.append(argument)\n",
    "                for _, argument in single_call[\"keyword\"].items():\n",
    "                    arguments.append(argument)\n",
    "                \n",
    "                if single_call.get(\"path\", False):\n",
    "                    if single_call[\"path\"].get(\"function_returns\", False):\n",
    "                        for end_node in single_call[\"path\"].get(\"function_returns\"):\n",
    "                            if arguments[0] in all_node_names and end_node in all_node_names:\n",
    "                                conditional_edges.append({\n",
    "                                    \"resolved\": True,\n",
    "                                    \"start_node\": arguments[0],\n",
    "                                    \"end_node\": end_node\n",
    "                                })\n",
    "                    elif single_call[\"path\"].get(\"function_fq_name\", False):\n",
    "                        conditional_edges.append({\n",
    "                            \"resolved\": False,\n",
    "                            \"function_fq_name\": single_call[\"path\"].get(\"function_fq_name\")\n",
    "                        })\n",
    "                elif single_call.get(\"path_map\", False):\n",
    "                    if single_call[\"path_map\"].get(\"list_values\", False):\n",
    "                        for end_node in single_call[\"path_map\"].get(\"list_values\"):\n",
    "                            if arguments[0] in all_node_names and end_node in all_node_names:\n",
    "                                conditional_edges.append({\n",
    "                                    \"resolved\": True,\n",
    "                                    \"start_node\": arguments[0],\n",
    "                                    \"end_node\": end_node\n",
    "                                })\n",
    "                    elif single_call[\"path_map\"].get(\"dict_values\", False):\n",
    "                        for end_node in single_call[\"path_map\"].get(\"dict_values\"):\n",
    "                            if arguments[0] in all_node_names and end_node in all_node_names:\n",
    "                                conditional_edges.append({\n",
    "                                    \"resolved\": True,\n",
    "                                    \"start_node\": arguments[0],\n",
    "                                    \"end_node\": end_node\n",
    "                                })\n",
    "                    elif single_call[\"path_map\"].get(\"map_fq_name\", False):\n",
    "                        conditional_edges.append({\n",
    "                            \"resolved\": False,\n",
    "                            \"map_fq_name\": single_call[\"path_map\"].get(\"map_fq_name\")\n",
    "                        })\n",
    "        \n",
    "        graphs.append({\n",
    "            \"graph_name\": graph,\n",
    "            \"graph_file_path\": call_records[\"filepath\"],\n",
    "            \"graph_info\": {\n",
    "                \"nodes\": nodes,\n",
    "                \"basic_edges\": basic_edges,\n",
    "                \"conditional_edges\": conditional_edges\n",
    "            }\n",
    "        })\n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = parse_all_graph_instances_in_directory(root_directory, graph_class_fqcn, add_conditional_edges_method_name, add_node_method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'graph_name': 'workflow', 'graph_file_path': './Gladiator2\\\\graph.py', 'graph_info': {'nodes': [{'name': 'input_node', 'definition': {'original': 'input_node', 'fq_name': 'nodes.nodes.input_node'}}, {'name': 'researcher', 'definition': {'original': 'call_model', 'fq_name': 'nodes.researcher.call_model'}}, {'name': 'tools', 'definition': {'original': 'tool_node', 'fq_name': 'nodes.nodes.tool_node'}}, {'name': 'generator', 'definition': {'original': 'generate_document_node', 'fq_name': 'nodes.generator.generate_document_node'}}, {'name': 'output_node', 'definition': {'original': 'output_node', 'fq_name': 'nodes.nodes.output_node'}}], 'basic_edges': [], 'conditional_edges': []}}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Gladiator2\\\\api.py', './Gladiator2\\\\graph.py', './Gladiator2\\\\legal_chatbot.py', './Gladiator2\\\\__init__.py', './Gladiator2\\\\example\\\\dicts.py', './Gladiator2\\\\example\\\\func.py', './Gladiator2\\\\LLMs\\\\llm.py', './Gladiator2\\\\LLMs\\\\__init__.py', './Gladiator2\\\\nodes\\\\generator.py', './Gladiator2\\\\nodes\\\\nodes.py', './Gladiator2\\\\nodes\\\\researcher.py', './Gladiator2\\\\nodes\\\\state.py', './Gladiator2\\\\nodes\\\\__init__.py', './Gladiator2\\\\prompts\\\\prompts.py', './Gladiator2\\\\prompts\\\\__init__.py', './Gladiator2\\\\Tools\\\\create_tools.py', './Gladiator2\\\\Tools\\\\helper.py', './Gladiator2\\\\Tools\\\\tools.py', './Gladiator2\\\\Tools\\\\__init__.py']\n"
     ]
    }
   ],
   "source": [
    "def get_python_files(directory):\n",
    "    python_files = []\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Check if the file has a .py extension\n",
    "            if file.endswith('.py'):\n",
    "                # Append the full path to the list\n",
    "                python_files.append(os.path.join(root, file))\n",
    "    return python_files\n",
    "\n",
    "print(get_python_files(root_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gotos(node, graph_file_path, root_directory):\n",
    "    \n",
    "    with open(graph_file_path, \"r\") as f:\n",
    "        code = f.read()\n",
    "\n",
    "    system_prompt = \"\"\"# Role Definition\n",
    "You are an analyzer of Python functions.\n",
    "Your goal is to detect if a Python function returns a specific type of object and then provide a list of all the possible values of a certain argument of that object.\n",
    "You are currently looking for returning the Command object, and you need to provide a list of all the possible values of the \"goto\" argument of that object.\n",
    "\n",
    "# Instructions\n",
    "In the first user message you will be provided with the following information:\n",
    "- the code file in which the function is used as an argument to a \"add_node\" method between the # Code Beginning and # Code End separators\n",
    "- the path to that file between the # Code File Path Beginning and # Code File Path End separators\n",
    "- the name of the function as it is passed to the \"add_node\" method, or the AST dump of the function as it is passed to the \"add_node\" method between the # Function Original Name Beginning and # Function Original Name End separators\n",
    "- the fully qualified name of the function between the # Fully Qualified Name Beginning and # Fully Qualified Name End separators\n",
    "- the paths to all of the available files in the directory between the # File Paths Beginning and # File Paths End separators\n",
    "\n",
    "# Task\n",
    "Your job is to look for the definition of the function in the code files and see if the function returns any Command objects.\n",
    "If the function returns any Command objects you must provide a list of all the possible values of the Command object's \"goto\" argument. The values of the \"goto\" argument can only be strings, END or START.\n",
    "Some functions do not return any Command objects, and in those cases you should provide an empty list.\n",
    "Only focus on the single function you were assigned in the first user message.\n",
    "\n",
    "# Response format\n",
    "You must respond with a JSON containing the following fields:\n",
    "1. A \"need_more_information\" field\". This is a boolean and it should be true if you need to see the code from some of the other files in the directory in order to solve the task, and false otherwise.\n",
    "2. A \"file_paths\" filed. This is a list of strings of the file paths of files from which you need to see the code. These can only be file paths from between the # File Paths Beginning and # File Paths End separators in the first user message. It should be an empty list if the \"need_more_information\" field is set to false.\n",
    "3. A \"goto_values\" list. This should contain all the possible values of the \"goto\" argument of the Command objects. If the function does not return any Command objects, this should be an empty list.\"\"\"\n",
    "\n",
    "    client = openai.AzureOpenAI()\n",
    "\n",
    "    first_user_message = f\"\"\"\n",
    "# Code Beginning\n",
    "{code}\n",
    "# Code End\n",
    "\n",
    "# Code File Path Beginning\n",
    "{graph_file_path}\n",
    "# Code File Path End\n",
    "\n",
    "# Function Original Name Beginning\n",
    "{node[\"definition\"][\"original\"]}\n",
    "# Function Original Name End\n",
    "\n",
    "# Fully Qualified Name Beginning\n",
    "{node[\"definition\"][\"fq_name\"]}\n",
    "# Fully Qualified Name End\n",
    "\n",
    "# File Paths Beginning\n",
    "{get_python_files(root_directory)}\n",
    "# File Paths End\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    messages= [\n",
    "        {\"role\":\"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": first_user_message}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages= messages,\n",
    "        response_format={\"type\":\"json_object\"}\n",
    "    )\n",
    "\n",
    "    response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    print(response_json)\n",
    "\n",
    "    need_more_information = response_json[\"need_more_information\"]\n",
    "\n",
    "    while need_more_information:\n",
    "        new_user_message = \"\"\n",
    "        for file_path in response_json[\"file_paths\"]:\n",
    "\n",
    "            with open(file_path, \"r\") as f:\n",
    "                code = f.read()\n",
    "                new_user_message+=f\"\"\"\n",
    "# Code Beginning\n",
    "{code}\n",
    "# Code End\n",
    "\n",
    "# Code File Path Beginning\n",
    "{file_path}\n",
    "# Code File Path End\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": response.choices[0].message.content\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": new_user_message\n",
    "        })\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"gpt-4o\",\n",
    "            messages= messages,\n",
    "            response_format={\"type\":\"json_object\"}\n",
    "        )\n",
    "\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        print(response_json)\n",
    "\n",
    "        need_more_information = response_json[\"need_more_information\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for graph in results:\n",
    "\n",
    "#     print(f\"Processing nodes from {graph[\"graph_name\"]}\")\n",
    "\n",
    "#     for node in graph[\"graph_info\"][\"nodes\"]:\n",
    "#         print(f\"Node: {node[\"name\"]}\")\n",
    "#         parse_gotos(node, graph[\"graph_file_path\"], root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool_access(node, graph_file_path, root_directory, predefined_tools, custom_tools):\n",
    "    \n",
    "    with open(graph_file_path, \"r\") as f:\n",
    "        code = f.read()\n",
    "\n",
    "    system_prompt = \"\"\"# Role Definition\n",
    "You are an analyzer of Python functions.\n",
    "Your goal is to fully trace the execution of a Python function, following every function call, class instantiation, method invocation, and variable definition, even when they lead to other source code files. Furthermore, if any of these definitions or calls contain other definitions or calls, you must track them recursively. You are finished with this once you have exhausted and followed all the definitions and calls, ensuring a comprehensive understanding of all dependencies and interactions within the codebase.\n",
    "You are doing this in order to understand which predefined tools and custom tools the function uses.\n",
    "Predefined tools are tools that are already defined in external libraries or modules, and are used by importing them.\n",
    "Custom tools are functions that are decorated with the @tool decorator, and that are defined within this codebase.\n",
    "\n",
    "# Instructions\n",
    "In the first user message you will be provided with the following information:\n",
    "- the code file in which the function is used as an argument to a \"add_node\" method between the # Code Beginning and # Code End separators\n",
    "- the path to that file between the # Code File Path Beginning and # Code File Path End separators\n",
    "- the name of the function as it is passed to the \"add_node\" method, or the AST dump of the function as it is passed to the \"add_node\" method between the # Function Original Name Beginning and # Function Original Name End separators\n",
    "- the fully qualified name of the function between the # Fully Qualified Name Beginning and # Fully Qualified Name End separators\n",
    "- the paths to all of the available files in the directory between the # File Paths Beginning and # File Paths End separators\n",
    "- a list of all the predefined tools whose usage has been detected in the directory between the # Predefined Tools List Beginning and # Predefined Tools List End separators\n",
    "- a list of all the custom tools that are defined in the directory between the # Custom Tools List Beginning and # Custom Tools List End separators\n",
    "\n",
    "# Task\n",
    "Your job is to look for the definition of the function in the code files and then fully trace it's execution in order to understand which predefined tools and custom tools the function uses.\n",
    "Predefined Tools will be listed between the # Predefined Tools List Beginning and # Predefined Tools List End separators. Each predefined tool in the list will be defined by the tool's name, the category to which the tool belongs, and the imports needed to use the predefined tool. The imports should help you detect where a predefined tool is used.\n",
    "Custom Tools will be listed between the # Custom Tools List Beginning and # Custom Tools List End separators. Each custom tool in the list will be defined by the tool's name (which is the name of the function decorated with the @tool decorator), the category to which the tool belongs (which is always \"CUSTOM\" for custom tools), the fully qualified name of the custom tool, and the file path of the file in which the custom tool is defined.\n",
    "Pay attention to tools from the \"LLM\" category of tools because you need to detect if these tools ever have the \"bind_tools\" method called on them.\n",
    "Some functions do not use any predefined or custom tools.\n",
    "Only focus on the single function you were assigned in the first user message.\n",
    "\n",
    "# Possible Scenarios\n",
    "1. The first possible scenario is that a function reference is passed to the \"add_node\" method. In this case you must simply find the defintion of the function that has been passed and trace its execution.\n",
    "2. The second possible scenario is that a function call is passed to the \"add_node\" method. In this case you must focus on the function or class that the function call returns, and trace its execution.\n",
    "3. If a ToolNode object is passed to the \"add_node\" method, or returned by the function call passed to the \"add_node\" method, then you must find the list of tools that the ToolNode uses.\n",
    "4. If any other class is passed to the \"add_node\" method, or is returned by the function call passed to the \"add_node\" method, then you must trace the execution of the __call__ method of that class.\n",
    "\n",
    "# Response format\n",
    "You must respond with a JSON containing the following fields:\n",
    "1. A \"notes\" field. This is a short note to yourself in the future in which you explain which function, class, method or variable definitions you still have to look at in order to solve the task, and explain your thoughts about the task so far. Keep this precise, as you will rely on this in the future.\n",
    "2. A \"need_more_information\" field. This is a boolean and it should be true if you need to see the code from some of the other files in the directory in order to solve the task, and false otherwise. The code from the requested files will be provided in the next user message.\n",
    "3. A \"file_paths\" filed. This is a list of strings of the file paths of files from which you need to see the code. These can only be file paths from between the # File Paths Beginning and # File Paths End separators in the first user message. It should be an empty list if the \"need_more_information\" field is set to false.\n",
    "4. A \"predefined_tools\" list. This should contain JSONs of all the predefined tools from the list of predefined tools in the first user message, and information about whether or not the function uses them. Each tool JSON should contain six fields. The first field is \"ordinal_id\" which is the same as the predefined tool's ordinal id in the list of predefined tools. You can use this to keep track of whether you listed all of the tools from the predefined tools list. The second field is \"name\", which should contain the name of the tool from the list of predefined tools. The third field is \"category\", which should contain the category of the tool from the list of predefined tools. The fourth field is \"bind_tools\", which should be true if the tool is from the \"LLM\" category and has the \"bind_tools\" method called, and false in all other cases. The fifth field is \"usage_explanation\" in which you explain the execution trace through which the function uses this predefined tool, or not if it doesn't. The sixth field is \"used\", which is a boolean that should be set to True if the function uses this predefined tool, and False if it does not.\n",
    "5. A \"custom_tools\" list. This should contain JSONs of all the custom tools from the list of custom tools in the first user message, and information about whether or not the function uses them. Each tool JSON should contain four fields. The first field is \"ordinal_id\" which is the same as the custom tool's ordinal id in the list of custom tools. You can use this to keep track of whether you listed all of the tools from the custom tools list. The second field is \"name\", which should contain the name of the tool from the list of custom tools. The third field is \"usage_explanation\" in which you explain the execution trace through which the function uses this custom tool, or not if it doesn't. The fourth field is \"used\", which is a boolean that should be set to True if the function uses this custom tool, and False if it does not.\n",
    "\"\"\"\n",
    "\n",
    "    client = openai.AzureOpenAI()\n",
    "\n",
    "    first_user_message = f\"\"\"\n",
    "# Code Beginning\n",
    "{code}\n",
    "# Code End\n",
    "\n",
    "# Code File Path Beginning\n",
    "{graph_file_path}\n",
    "# Code File Path End\n",
    "\n",
    "# Function Original Name Beginning\n",
    "{node[\"definition\"][\"original\"]}\n",
    "# Function Original Name End\n",
    "\n",
    "# Fully Qualified Name Beginning\n",
    "{node[\"definition\"][\"fq_name\"]}\n",
    "# Fully Qualified Name End\n",
    "\n",
    "# File Paths Beginning\n",
    "{get_python_files(root_directory)}\n",
    "# File Paths End\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    first_user_message+=f\"\"\"# Predefined Tools List Beginning\n",
    "\"\"\"\n",
    "    for i, predefined_tool in enumerate(predefined_tools):\n",
    "\n",
    "        first_user_message+=f\"\"\"{i+1}. Tool Name: {predefined_tool[\"name\"]}\n",
    "Tool Category: {predefined_tool[\"category\"]}\n",
    "Tool Imports: {predefined_tool[\"imports\"]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    first_user_message+=f\"\"\"# Predefined Tools List End\n",
    "\"\"\"\n",
    "    \n",
    "    first_user_message+=f\"\"\"# Custom Tools List Beginning\n",
    "\"\"\"\n",
    "    for i, custom_tool in enumerate(custom_tools):\n",
    "\n",
    "        first_user_message+=f\"\"\"{i+1}. Tool Name: {custom_tool[\"name\"]}\n",
    "Tool Category: CUSTOM\n",
    "Tool Fully Qualified Name: {custom_tool[\"fully_qualified_name\"]}\n",
    "Tool Fully File Path: {custom_tool[\"filepath\"]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    first_user_message+=f\"\"\"# Custom Tools List End\n",
    "\"\"\"\n",
    "\n",
    "    messages= [\n",
    "        {\"role\":\"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": first_user_message}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages= messages,\n",
    "        response_format={\"type\":\"json_object\"}\n",
    "    )\n",
    "\n",
    "    response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    print(response_json)\n",
    "\n",
    "    need_more_information = response_json[\"need_more_information\"]\n",
    "\n",
    "    while need_more_information:\n",
    "        new_user_message = \"\"\n",
    "        for file_path in response_json[\"file_paths\"]:\n",
    "\n",
    "            with open(file_path, \"r\") as f:\n",
    "                code = f.read()\n",
    "                new_user_message+=f\"\"\"\n",
    "# Code Beginning\n",
    "{code}\n",
    "# Code End\n",
    "\n",
    "# Code File Path Beginning\n",
    "{file_path}\n",
    "# Code File Path End\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": response.choices[0].message.content\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": new_user_message\n",
    "        })\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"gpt-4o\",\n",
    "            messages= messages,\n",
    "            response_format={\"type\":\"json_object\"}\n",
    "        )\n",
    "\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        print(response_json)\n",
    "\n",
    "        need_more_information = response_json[\"need_more_information\"]\n",
    "    \n",
    "    for predefined_tool in response_json[\"predefined_tools\"]:\n",
    "        if predefined_tool[\"used\"]:\n",
    "            print(predefined_tool[\"name\"])\n",
    "\n",
    "    for custom_tool in response_json[\"custom_tools\"]:\n",
    "        if custom_tool[\"used\"]:\n",
    "            print(custom_tool[\"name\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nodes from workflow\n",
      "Node: input_node\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[475], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_info\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mparse_tool_access\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraph_file_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_tools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_tools\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[474], line 103\u001b[0m, in \u001b[0;36mparse_tool_access\u001b[1;34m(node, graph_file_path, root_directory, predefined_tools, custom_tools)\u001b[0m\n\u001b[0;32m     95\u001b[0m     first_user_message\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m# Custom Tools List End\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     messages\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     99\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[0;32m    100\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: first_user_message}\n\u001b[0;32m    101\u001b[0m     ]\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson_object\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response_json)\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\openai\\_base_client.py:996\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m    993\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 996\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1002\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\ssl.py:1232\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1230\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1231\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for graph in results:\n",
    "\n",
    "    print(f\"Processing nodes from {graph[\"graph_name\"]}\")\n",
    "\n",
    "    for node in graph[\"graph_info\"][\"nodes\"]:\n",
    "        print(f\"Node: {node[\"name\"]}\")\n",
    "        parse_tool_access(node, graph[\"graph_file_path\"], root_directory, possible_tools, custom_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool_access(node, graph_file_path, root_directory, predefined_tools, custom_tools):\n",
    "    \n",
    "    with open(graph_file_path, \"r\") as f:\n",
    "        code = f.read()\n",
    "\n",
    "    system_prompt = \"\"\"# Role Definition\n",
    "You are an analyzer of Python functions.\n",
    "Your goal is split into two parts:\n",
    "1. The first part is to fully trace the execution of a function. This means that you should look at the functions definition, identify all the functions, classes, methods, and variables that the function uses, and find the definitions of these functions, classes, methods, and variables by asking to see the content of the code files in which they are defined. Furthermore, if these components contain any other functions, classes, methods, and variables, you should continue this process recursively until you have access to all the definitions.\n",
    "2. The second part is seeing which predefined tools and custom tools, if any, the function uses during its execution. You will be provided with the information needed to detect these tools.\n",
    "Predefined tools are tools that are already defined in external libraries or modules, and are used by importing them.\n",
    "Custom tools are functions that are decorated with the @tool decorator, and that are defined within this codebase.\n",
    "\n",
    "# Instructions\n",
    "In the first user message you will be provided with the following information:\n",
    "- the code file in which the function is used as an argument to a \"add_node\" method between the # Code Beginning and # Code End separators\n",
    "- the path to that file between the # Code File Path Beginning and # Code File Path End separators\n",
    "- the name of the function as it is passed to the \"add_node\" method, or the AST dump of the function as it is passed to the \"add_node\" method between the # Function Original Name Beginning and # Function Original Name End separators\n",
    "- the fully qualified name of the function between the # Fully Qualified Name Beginning and # Fully Qualified Name End separators\n",
    "- the paths to all of the available files in the directory between the # File Paths Beginning and # File Paths End separators\n",
    "- a list of all the predefined tools whose usage has been detected in the directory between the # Predefined Tools List Beginning and # Predefined Tools List End separators\n",
    "- a list of all the custom tools that are defined in the directory between the # Custom Tools List Beginning and # Custom Tools List End separators\n",
    "\n",
    "# Task\n",
    "Your job is to look for the definition of the function in the code files and then fully trace its execution in order to understand which predefined tools and custom tools the function uses.\n",
    "Predefined Tools will be listed between the # Predefined Tools List Beginning and # Predefined Tools List End separators. Each predefined tool in the list will be defined by the tool's name, the category to which the tool belongs, and the imports needed to use the predefined tool. The imports should help you detect where a predefined tool is used.\n",
    "Custom Tools will be listed between the # Custom Tools List Beginning and # Custom Tools List End separators. Each custom tool in the list will be defined by the tool's name (which is the name of the function decorated with the @tool decorator), the category to which the tool belongs (which is always \"CUSTOM\" for custom tools), the fully qualified name of the custom tool, and the file path of the file in which the custom tool is defined.\n",
    "Pay attention to tools from the \"LLM\" category of predefined tools because you need to detect if these tools ever have the \"bind_tools\" method called on them.\n",
    "Some functions do not use any predefined or custom tools.\n",
    "Only focus on the execution of the single function you were assigned in the first user message.\n",
    "\n",
    "# Possible Scenarios\n",
    "These are the scenarios you might encounter while looking at the \"add_node\" method:\n",
    "1. The first possible scenario is that a function reference is passed to the \"add_node\" method. In this case you must simply find the defintion of the function that has been passed and trace its execution.\n",
    "2. The second possible scenario is that a function call is passed to the \"add_node\" method. In this case you must focus on the function or class that the function call returns, and trace its execution.\n",
    "3. If a ToolNode object is passed to the \"add_node\" method, or returned by the function call passed to the \"add_node\" method, then you must find the list of tools that the ToolNode uses.\n",
    "4. If any other class is passed to the \"add_node\" method, or is returned by the function call passed to the \"add_node\" method, then you must trace the execution of the __call__ method of that class.\n",
    "\n",
    "# Response format\n",
    "You must respond with a JSON containing the following fields:\n",
    "1. A \"notes\" field. This is a short note to yourself in the future in which you explain which function, class, method or variable definitions you still have to look at in order to solve the task, and explain your thoughts about the task so far. Keep this precise, as you will rely on this in the future.\n",
    "2. A \"need_more_information\" field. This is a boolean and it should be true if you need to see the code from some of the other files in the directory in order to solve the task, and false otherwise.\n",
    "3. A \"file_paths\" filed. This is a list of strings of the file paths of files from which you need to see the code. These can only be file paths from between the # File Paths Beginning and # File Paths End separators in the first user message. It should be an empty list if the \"need_more_information\" field is set to false. The code from the requested files will be provided in the next user message.\n",
    "4. A \"predefined_tools\" list. This should contain JSONs of all the predefined tools from the list of predefined tools in the first user message, and information about whether or not the function uses them. Each tool JSON should contain six fields. The first field is \"ordinal_id\" which is the same as the predefined tool's ordinal id in the list of predefined tools. You can use this to keep track of whether you listed all of the tools from the predefined tools list. The second field is \"name\", which should contain the name of the tool from the list of predefined tools. The third field is \"category\", which should contain the category of the tool from the list of predefined tools. The fourth field is \"bind_tools\", which should be true if the tool is from the \"LLM\" category and has the \"bind_tools\" method called, and false in all other cases. The fifth field is \"usage_explanation\" in which you explain the execution trace through which the function uses this predefined tool, or not if it doesn't. The sixth field is \"used\", which is a boolean that should be set to True if the function uses this predefined tool, and False if it does not. This should be an empty list if the \"need_more_information\" field is set to True.\n",
    "5. A \"custom_tools\" list. This should contain JSONs of all the custom tools from the list of custom tools in the first user message, and information about whether or not the function uses them. Each tool JSON should contain four fields. The first field is \"ordinal_id\" which is the same as the custom tool's ordinal id in the list of custom tools. You can use this to keep track of whether you listed all of the tools from the custom tools list. The second field is \"name\", which should contain the name of the tool from the list of custom tools. The third field is \"usage_explanation\" in which you explain the execution trace through which the function uses this custom tool, or not if it doesn't. The fourth field is \"used\", which is a boolean that should be set to True if the function uses this custom tool, and False if it does not. This should be an empty list if the \"need_more_information\" field is set to True.\n",
    "\"\"\"\n",
    "\n",
    "    client = openai.AzureOpenAI()\n",
    "\n",
    "    first_user_message = f\"\"\"\n",
    "# Code Beginning\n",
    "{code}\n",
    "# Code End\n",
    "\n",
    "# Code File Path Beginning\n",
    "{graph_file_path}\n",
    "# Code File Path End\n",
    "\n",
    "# Function Original Name Beginning\n",
    "{node[\"definition\"][\"original\"]}\n",
    "# Function Original Name End\n",
    "\n",
    "# Fully Qualified Name Beginning\n",
    "{node[\"definition\"][\"fq_name\"]}\n",
    "# Fully Qualified Name End\n",
    "\n",
    "# File Paths Beginning\n",
    "{get_python_files(root_directory)}\n",
    "# File Paths End\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    first_user_message+=f\"\"\"# Predefined Tools List Beginning\n",
    "\"\"\"\n",
    "    for i, predefined_tool in enumerate(predefined_tools):\n",
    "\n",
    "        first_user_message+=f\"\"\"{i+1}. Tool Name: {predefined_tool[\"name\"]}\n",
    "Tool Category: {predefined_tool[\"category\"]}\n",
    "Tool Imports: {predefined_tool[\"imports\"]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    first_user_message+=f\"\"\"# Predefined Tools List End\n",
    "\"\"\"\n",
    "    \n",
    "    first_user_message+=f\"\"\"# Custom Tools List Beginning\n",
    "\"\"\"\n",
    "    for i, custom_tool in enumerate(custom_tools):\n",
    "\n",
    "        first_user_message+=f\"\"\"{i+1}. Tool Name: {custom_tool[\"name\"]}\n",
    "Tool Category: CUSTOM\n",
    "Tool Fully Qualified Name: {custom_tool[\"fully_qualified_name\"]}\n",
    "Tool Fully File Path: {custom_tool[\"filepath\"]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    first_user_message+=f\"\"\"# Custom Tools List End\n",
    "\"\"\"\n",
    "\n",
    "    messages= [\n",
    "        {\"role\":\"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": first_user_message}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages= messages,\n",
    "        response_format={\"type\":\"json_object\"}\n",
    "    )\n",
    "\n",
    "    response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    print(response_json)\n",
    "\n",
    "    need_more_information = response_json[\"need_more_information\"]\n",
    "\n",
    "    while need_more_information:\n",
    "        new_user_message = \"\"\n",
    "        for file_path in response_json[\"file_paths\"]:\n",
    "\n",
    "            with open(file_path, \"r\") as f:\n",
    "                code = f.read()\n",
    "                new_user_message+=f\"\"\"\n",
    "# Code Beginning\n",
    "{code}\n",
    "# Code End\n",
    "\n",
    "# Code File Path Beginning\n",
    "{file_path}\n",
    "# Code File Path End\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": response.choices[0].message.content\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": new_user_message\n",
    "        })\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"gpt-4o\",\n",
    "            messages= messages,\n",
    "            response_format={\"type\":\"json_object\"}\n",
    "        )\n",
    "\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        print(response_json)\n",
    "\n",
    "        need_more_information = response_json[\"need_more_information\"]\n",
    "    \n",
    "    for predefined_tool in response_json[\"predefined_tools\"]:\n",
    "        if predefined_tool[\"used\"]:\n",
    "            print(predefined_tool[\"name\"])\n",
    "\n",
    "    for custom_tool in response_json[\"custom_tools\"]:\n",
    "        if custom_tool[\"used\"]:\n",
    "            print(custom_tool[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nodes from workflow\n",
      "Node: input_node\n",
      "{'notes': \"I need to examine the definition of the 'input_node' function in the 'nodes/nodes.py' file to trace its execution and determine which predefined and custom tools it uses.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/nodes/nodes.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I have found the definition of the 'input_node' function. To fully trace its execution, I need to check the 'add_documents' method of the 'legal_docs_vector_store_class' in the 'Tools/tools.py' file because it is used by the 'input_node' function.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/Tools/tools.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I have identified the definition of the 'legal_docs_vector_store_class' and the 'HuggingFaceEmbeddings' tool as well as the 'TavilySearchResults' predefined tools' usage.  Since the 'legal_docs_vector_store_class' uses 'LegalDocumentsVectorStore' from 'Tools.helper', I need to check the implementation of this class to fully trace its execution and understand the 'add_documents' method.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/Tools/helper.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I have traced the execution of the 'input_node' function. The 'input_node' function uses the 'legal_docs_vector_store_class' which is an instance of 'LegalDocumentsVectorStore'. The 'add_documents' method of 'LegalDocumentsVectorStore' has been traced fully. The 'HuggingFaceEmbeddings' predefined tool is used in this method and 'PineconeVectorStore' is also involved.\", 'need_more_information': False, 'file_paths': [], 'predefined_tools': [{'ordinal_id': 2, 'name': 'Tavily Search', 'category': 'GENERAL', 'bind_tools': False, 'usage_explanation': 'TavilySearchResults is initialized and added to the tools list which is utilized in the code.', 'used': True}, {'ordinal_id': 3, 'name': 'Azure OpenAI', 'category': 'LLM', 'bind_tools': False, 'usage_explanation': '', 'used': False}, {'ordinal_id': 4, 'name': 'AWS Bedrock', 'category': 'LLM', 'bind_tools': False, 'usage_explanation': '', 'used': False}, {'ordinal_id': 5, 'name': 'Pinecone', 'category': 'VECTOR_STORE', 'bind_tools': False, 'usage_explanation': 'PineconeVectorStore is used within the LegalDocumentsVectorStore class which is instantiated and used in input_node function.', 'used': True}], 'custom_tools': []}\n",
      "Tavily Search\n",
      "Pinecone\n",
      "Node: researcher\n",
      "{'notes': \"I need to find the definition of the 'call_model' function in the 'nodes.researcher' module. Once I get the definition, I need to trace its execution to determine if it uses any predefined or custom tools.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/nodes/researcher.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I have found the definition of the 'call_model' function in the 'nodes.researcher' module. I now need to trace its execution to determine if it uses any predefined or custom tools. Additionally, I need to look into the 'azure_llm' and 'tools' objects as they are used within the 'call_model' function. 'azure_llm' is likely from './Gladiator2/LLMs/llm.py' and 'tools' is likely from './Gladiator2/Tools/tools.py'.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/LLMs/llm.py', './Gladiator2/Tools/tools.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I have obtained the definition of 'azure_llm' and 'tools'. 'azure_llm' is an instance of the AzureChatOpenAI which is a predefined tool from the 'LLM' category and it has the 'bind_tools' method called with the 'tools' list. The 'tools' list includes the predefined tool 'Tavily Search' and the custom tool 'generate_document_parsed_input_json_tool'. I now need to confirm how these are used in 'call_model'.\", 'need_more_information': False, 'file_paths': [], 'predefined_tools': [{'ordinal_id': 3, 'name': 'Azure OpenAI', 'category': 'LLM', 'bind_tools': True, 'usage_explanation': \"The 'call_model' function calls 'azure_llm.invoke' after binding tools, which include 'Tavily Search'.\", 'used': True}, {'ordinal_id': 2, 'name': 'Tavily Search', 'category': 'GENERAL', 'bind_tools': False, 'usage_explanation': \"The 'Tavily Search' tool is part of the 'tools' list that was bound to 'azure_llm' which is used in 'call_model'.\", 'used': True}], 'custom_tools': [{'ordinal_id': 1, 'name': 'generate_document_parsed_input_json_tool', 'usage_explanation': \"The 'generate_document_parsed_input_json_tool' is part of the 'tools' list that was bound to 'azure_llm' which is used in 'call_model'.\", 'used': True}]}\n",
      "Azure OpenAI\n",
      "Tavily Search\n",
      "generate_document_parsed_input_json_tool\n",
      "Node: tools\n",
      "{'notes': \"I need to find the definition of the function 'tool_node' in the file './Gladiator2/nodes/nodes.py'. Once I have that, I will examine its execution to trace all functions, classes, methods, and variables that it uses. This will also help in identifying any predefined or custom tools being utilized.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/nodes/nodes.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I found the definition of the 'tool_node' function. Now I need to make sure to fully trace its execution, and that involves checking the 'tools' and 'legal_docs_vector_store_class' from './Gladiator2/Tools/tools.py'. Also, I need to check the 'ToolMessage' import from 'langchain_core.messages', 'END' from 'langgraph.graph', and 'Command' from 'langgraph.types' to ensure there's no nested call to predefined or custom tools inside them.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/Tools/tools.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"The 'tool_node' function uses the 'tools' list from './Gladiator2/Tools/tools.py'. The tools in this list are 'TavilySearchResults' from 'langchain_community.tools', 'legal_docs_retriever_tool', 'employee_db_tool', 'jira_search_tool', and 'generate_document_parsed_input_json_tool'. I still need to fully trace these tools to know if they use any predefined or custom tools, especially focusing on any methods utilized in their classes. Additionally, I need to find the definition and follow the usage of 'legal_docs_vector_store_class' inside './Gladiator2/Tools/tools.py' to see if it engages any predefined or custom tools.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/Tools/create_tools.py', './Gladiator2/Tools/helper.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I have found all necessary definitions related to the 'tool_node' function. I can now confirm which predefined and custom tools it uses. Specifically, the function uses several custom tools from './Gladiator2/Tools/create_tools.py', such as 'generate_document_parsed_input_json_tool'. It also checks predefined tools such as 'TavilySearchResults' and 'JiraAPIWrapper', imported from 'langchain_community.utilities.jira' and 'langchain_community.tools'. Additionally, the 'legal_docs_vector_store_class' uses 'PineconeVectorStore' from 'langchain_pinecone', which makes it a predefined tool usage.\", 'need_more_information': False, 'file_paths': [], 'predefined_tools': [{'ordinal_id': 1, 'name': 'Jira Toolkit', 'category': 'GENERAL', 'bind_tools': False, 'usage_explanation': \"The 'tool_node' function utilizes 'jira_search_tool' and indirectly 'JiraAPIWrapper' from 'langchain_community.utilities.jira'.\", 'used': True}, {'ordinal_id': 2, 'name': 'Tavily Search', 'category': 'GENERAL', 'bind_tools': False, 'usage_explanation': \"The 'tool_node' function references 'tavily_search', which is an instance of 'TavilySearchResults' from 'langchain_community.tools'.\", 'used': True}, {'ordinal_id': 5, 'name': 'Pinecone', 'category': 'VECTOR_STORE', 'bind_tools': False, 'usage_explanation': \"The 'legal_docs_vector_store_class' used in 'tool_node' utilizes 'PineconeVectorStore' from 'langchain_pinecone'.\", 'used': True}, {'ordinal_id': 3, 'name': 'Azure OpenAI', 'category': 'LLM', 'bind_tools': False, 'usage_explanation': 'The function does not utilize Azure OpenAI.', 'used': False}, {'ordinal_id': 4, 'name': 'AWS Bedrock', 'category': 'LLM', 'bind_tools': False, 'usage_explanation': 'The function does not utilize AWS Bedrock.', 'used': False}], 'custom_tools': [{'ordinal_id': 1, 'name': 'generate_document_parsed_input_json_tool', 'usage_explanation': \"This tool is explicitly listed in the 'tools' list in './Gladiator2/Tools/tools.py', which is utilized by 'tool_node'.\", 'used': True}, {'ordinal_id': 2, 'name': 'retriever_tool', 'usage_explanation': \"This tool is explicitly listed in the 'tools' list in './Gladiator2/Tools/tools.py', which is utilized by 'tool_node'.\", 'used': True}, {'ordinal_id': 3, 'name': 'run_query', 'usage_explanation': \"This tool is not directly referenced in the 'tool_node' function or its dependencies.\", 'used': False}, {'ordinal_id': 4, 'name': 'search_issue', 'usage_explanation': \"The 'jira_search_tool', which uses the 'search_issue' tool is part of the 'tools' list in './Gladiator2/Tools/tools.py' used by 'tool_node'.\", 'used': True}, {'ordinal_id': 5, 'name': 'create_issue', 'usage_explanation': \"This tool is instantiated but not used in the 'tools' list in './Gladiator2/Tools/tools.py' and hence not in 'tool_node'.\", 'used': False}]}\n",
      "Jira Toolkit\n",
      "Tavily Search\n",
      "Pinecone\n",
      "generate_document_parsed_input_json_tool\n",
      "retriever_tool\n",
      "search_issue\n",
      "Node: generator\n",
      "{'notes': \"I need to look at the definition of the function 'generate_document_node' located in 'nodes/generator.py' to trace its execution and detect any predefined or custom tools it may use.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/nodes/generator.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I need to trace the imports and the call to 'aws_llm' in 'generate_document_node' to determine if any other functions or classes are involved in the execution. Additionally, I need to check details about 'retrieve_whole_document', 'save_document', 'jira_create_tool' and 'aws_llm'.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/LLMs/llm.py', './Gladiator2/Tools/helper.py', './Gladiator2/Tools/tools.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': \"I have now gathered the necessary information about the implementation of 'generate_document_node'. The function uses predefined tools such as 'Azure OpenAI' (azure_llm which is a AzureChatOpenAI instance with tools bound but not used directly in this function), 'AWS Bedrock' (aws_llm in this function). Custom tools such as 'jira_create_tool', 'retrieve_whole_document' and 'save_document' are also used in this function.\", 'need_more_information': False, 'file_paths': [], 'predefined_tools': [{'ordinal_id': 3, 'name': 'Azure OpenAI', 'category': 'LLM', 'bind_tools': True, 'usage_explanation': \"The function 'generate_document_node' includes 'azure_llm' which binds tools from 'Tools/tools.py', but these tools are not used directly in 'generate_document_node'.\", 'used': False}, {'ordinal_id': 4, 'name': 'AWS Bedrock', 'category': 'LLM', 'bind_tools': False, 'usage_explanation': \"The function 'generate_document_node' uses 'aws_llm' to invoke chat_prompt.\", 'used': True}], 'custom_tools': [{'ordinal_id': 4, 'name': 'search_issue', 'usage_explanation': \"Custom tool 'search_issue' is defined but not directly used in this function.\", 'used': False}, {'ordinal_id': 3, 'name': 'run_query', 'usage_explanation': \"Custom tool 'run_query' is defined but not directly used in this function.\", 'used': False}, {'ordinal_id': 2, 'name': 'retriever_tool', 'usage_explanation': \"Custom tool 'retriever_tool' is defined but not directly used in this function.\", 'used': False}, {'ordinal_id': 1, 'name': 'generate_document_parsed_input_json_tool', 'usage_explanation': \"Custom tool 'generate_document_parsed_input_json_tool' is defined but not directly used in this function.\", 'used': False}, {'ordinal_id': 5, 'name': 'create_issue', 'usage_explanation': \"Custom tool 'jira_create_tool', an instance of 'create_issue', is used in 'generate_document_node' when creating a JIRA ticket.\", 'used': True}]}\n",
      "AWS Bedrock\n",
      "create_issue\n",
      "Node: output_node\n",
      "{'notes': 'I need to look at the definition of the output_node function in the nodes/nodes.py file to trace its execution and identify any predefined or custom tools it uses.', 'need_more_information': True, 'file_paths': ['./Gladiator2/nodes/nodes.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': 'The output_node function calls the delete_documents method on the legal_docs_vector_store_class. I need to investigate the definition of the legal_docs_vector_store_class in the file Tools/tools.py to understand what it does.', 'need_more_information': True, 'file_paths': ['./Gladiator2/Tools/tools.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': 'The legal_docs_vector_store_class is defined as an instance of the LegalDocumentsVectorStore class from Tools.helper. I need to look at the definition of the LegalDocumentsVectorStore class in Tools/helper.py to understand the delete_documents method and any other relevant details.', 'need_more_information': True, 'file_paths': ['./Gladiator2/Tools/helper.py'], 'predefined_tools': [], 'custom_tools': []}\n",
      "{'notes': 'I have now reviewed the definition of LegalDocumentsVectorStore class. The delete_documents method deletes documents from the vector store. I can now conclude tracing the execution of the output_node function and check for predefined and custom tools.', 'need_more_information': False, 'file_paths': [], 'predefined_tools': [{'ordinal_id': 1, 'name': 'Jira Toolkit', 'category': 'GENERAL', 'bind_tools': False, 'usage_explanation': 'The function does not use the Jira Toolkit.', 'used': False}, {'ordinal_id': 2, 'name': 'Tavily Search', 'category': 'GENERAL', 'bind_tools': False, 'usage_explanation': 'The function does not use the Tavily Search.', 'used': False}, {'ordinal_id': 3, 'name': 'Azure OpenAI', 'category': 'LLM', 'bind_tools': False, 'usage_explanation': 'The function does not use the Azure OpenAI.', 'used': False}, {'ordinal_id': 4, 'name': 'AWS Bedrock', 'category': 'LLM', 'bind_tools': False, 'usage_explanation': 'The function does not use the AWS Bedrock.', 'used': False}, {'ordinal_id': 5, 'name': 'Pinecone', 'category': 'VECTOR_STORE', 'bind_tools': False, 'usage_explanation': 'The function does not use Pinecone directly.', 'used': False}], 'custom_tools': [{'ordinal_id': 1, 'name': 'generate_document_parsed_input_json_tool', 'usage_explanation': 'The function does not use this custom tool.', 'used': False}, {'ordinal_id': 2, 'name': 'retriever_tool', 'usage_explanation': 'The function does not use this custom tool.', 'used': False}, {'ordinal_id': 3, 'name': 'run_query', 'usage_explanation': 'The function does not use this custom tool.', 'used': False}, {'ordinal_id': 4, 'name': 'search_issue', 'usage_explanation': 'The function does not use this custom tool.', 'used': False}, {'ordinal_id': 5, 'name': 'create_issue', 'usage_explanation': 'The function does not use this custom tool.', 'used': False}]}\n"
     ]
    }
   ],
   "source": [
    "for graph in results:\n",
    "\n",
    "    print(f\"Processing nodes from {graph[\"graph_name\"]}\")\n",
    "\n",
    "    for node in graph[\"graph_info\"][\"nodes\"]:\n",
    "        print(f\"Node: {node[\"name\"]}\")\n",
    "        parse_tool_access(node, graph[\"graph_file_path\"], root_directory, possible_tools, custom_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool_access(node, graph_file_path, root_directory, predefined_tools, custom_tools):\n",
    "    \n",
    "    with open(graph_file_path, \"r\") as f:\n",
    "        code = f.read()\n",
    "\n",
    "    system_prompt = \"\"\"\"\"\"\n",
    "\n",
    "    client = openai.AzureOpenAI()\n",
    "\n",
    "    first_user_message = f\"\"\"\n",
    "# Code Beginning\n",
    "{code}\n",
    "# Code End\n",
    "\n",
    "# Code File Path Beginning\n",
    "{graph_file_path}\n",
    "# Code File Path End\n",
    "\n",
    "# Function Original Name Beginning\n",
    "{node[\"definition\"][\"original\"]}\n",
    "# Function Original Name End\n",
    "\n",
    "# Fully Qualified Name Beginning\n",
    "{node[\"definition\"][\"fq_name\"]}\n",
    "# Fully Qualified Name End\n",
    "\n",
    "# File Paths Beginning\n",
    "{get_python_files(root_directory)}\n",
    "# File Paths End\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    first_user_message+=f\"\"\"# Predefined Tools List Beginning\n",
    "\"\"\"\n",
    "    for i, predefined_tool in enumerate(predefined_tools):\n",
    "\n",
    "        first_user_message+=f\"\"\"{i+1}. Tool Name: {predefined_tool[\"name\"]}\n",
    "Tool Category: {predefined_tool[\"category\"]}\n",
    "Tool Imports: {predefined_tool[\"imports\"]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    first_user_message+=f\"\"\"# Predefined Tools List End\n",
    "\"\"\"\n",
    "    \n",
    "    first_user_message+=f\"\"\"# Custom Tools List Beginning\n",
    "\"\"\"\n",
    "    for i, custom_tool in enumerate(custom_tools):\n",
    "\n",
    "        first_user_message+=f\"\"\"{i+1}. Tool Name: {custom_tool[\"name\"]}\n",
    "Tool Category: CUSTOM\n",
    "Tool Fully Qualified Name: {custom_tool[\"fully_qualified_name\"]}\n",
    "Tool Fully File Path: {custom_tool[\"filepath\"]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    first_user_message+=f\"\"\"# Custom Tools List End\n",
    "\"\"\"\n",
    "\n",
    "    messages= [\n",
    "        {\"role\":\"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": first_user_message}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages= messages,\n",
    "        response_format={\"type\":\"json_object\"}\n",
    "    )\n",
    "\n",
    "    response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    print(response_json)\n",
    "\n",
    "    need_more_information = response_json[\"need_more_information\"]\n",
    "\n",
    "    while need_more_information:\n",
    "        new_user_message = \"\"\n",
    "        for file_path in response_json[\"file_paths\"]:\n",
    "\n",
    "            with open(file_path, \"r\") as f:\n",
    "                code = f.read()\n",
    "                new_user_message+=f\"\"\"\n",
    "# Code Beginning\n",
    "{code}\n",
    "# Code End\n",
    "\n",
    "# Code File Path Beginning\n",
    "{file_path}\n",
    "# Code File Path End\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": response.choices[0].message.content\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": new_user_message\n",
    "        })\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"gpt-4o\",\n",
    "            messages= messages,\n",
    "            response_format={\"type\":\"json_object\"}\n",
    "        )\n",
    "\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        print(response_json)\n",
    "\n",
    "        need_more_information = response_json[\"need_more_information\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nodes from workflow\n",
      "Node: input_node\n",
      "{'notes': \"I need to see the content of the file './Gladiator2/nodes/nodes.py' in order to trace the execution of the 'input_node'.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/nodes/nodes.py']}\n",
      "{'notes': \"The 'input_node' function uses 'legal_docs_vector_store_class' and 'Command' components. I need to see the content of the file './Gladiator2/Tools/tools.py' to understand 'legal_docs_vector_store_class', and './Gladiator2/langgraph/types.py' to understand 'Command'.\", 'need_more_information': True, 'file_paths': ['./Gladiator2/Tools/tools.py', './Gladiator2/langgraph/types.py']}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Gladiator2/langgraph/types.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[479], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_info\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mparse_tool_access\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraph_file_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_tools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_tools\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[478], line 114\u001b[0m, in \u001b[0;36mparse_tool_access\u001b[1;34m(node, graph_file_path, root_directory, predefined_tools, custom_tools)\u001b[0m\n\u001b[0;32m    111\u001b[0m         new_user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m response_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_paths\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 114\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    115\u001b[0m                 code \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    116\u001b[0m                 new_user_message\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124m# Code Beginning\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jurin\\anaconda3\\envs\\agentic_graph\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Gladiator2/langgraph/types.py'"
     ]
    }
   ],
   "source": [
    "for graph in results:\n",
    "\n",
    "    print(f\"Processing nodes from {graph[\"graph_name\"]}\")\n",
    "\n",
    "    for node in graph[\"graph_info\"][\"nodes\"]:\n",
    "        print(f\"Node: {node[\"name\"]}\")\n",
    "        parse_tool_access(node, graph[\"graph_file_path\"], root_directory, possible_tools, custom_tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
